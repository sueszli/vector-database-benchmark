[
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().setUp()\n    vocab = ['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', '\u0120', '\u0120l', '\u0120n', '\u0120lo', '\u0120low', 'er', '\u0120lowest', '\u0120newer', '\u0120wider', '\u0120hello', '\u0120world', '<unk>']\n    vocab_tokens = dict(zip(vocab, range(len(vocab))))\n    merges = ['#version: 0.2', '\u0120 l', '\u0120l o', '\u0120lo w', 'e r', '']\n    self.tags_dict = {'a': 0, 'abbr': 1, 'acronym': 2, 'address': 3}\n    self.special_tokens_map = {'unk_token': '<unk>'}\n    self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['vocab_file'])\n    self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES['merges_file'])\n    self.tokenizer_config_file = os.path.join(self.tmpdirname, 'tokenizer_config.json')\n    with open(self.vocab_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps(vocab_tokens) + '\\n')\n    with open(self.merges_file, 'w', encoding='utf-8') as fp:\n        fp.write('\\n'.join(merges))\n    with open(self.tokenizer_config_file, 'w', encoding='utf-8') as fp:\n        fp.write(json.dumps({'tags_dict': self.tags_dict}))"
        ]
    },
    {
        "func_name": "get_nodes_and_xpaths",
        "original": "def get_nodes_and_xpaths(self):\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)",
        "mutated": [
            "def get_nodes_and_xpaths(self):\n    if False:\n        i = 10\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nodes = ['hello', 'world']\n    xpaths = ['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span']\n    return (nodes, xpaths)"
        ]
    },
    {
        "func_name": "get_nodes_and_xpaths_batch",
        "original": "def get_nodes_and_xpaths_batch(self):\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)",
        "mutated": [
            "def get_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)",
            "def get_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (nodes, xpaths)"
        ]
    },
    {
        "func_name": "get_question_nodes_and_xpaths",
        "original": "def get_question_nodes_and_xpaths(self):\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)",
        "mutated": [
            "def get_question_nodes_and_xpaths(self):\n    if False:\n        i = 10\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)",
            "def get_question_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)",
            "def get_question_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)",
            "def get_question_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)",
            "def get_question_nodes_and_xpaths(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    question = \"what's his name?\"\n    nodes = ['hello world']\n    xpaths = ['/html/body/div/li[1]/div/span']\n    return (question, nodes, xpaths)"
        ]
    },
    {
        "func_name": "get_question_nodes_and_xpaths_batch",
        "original": "def get_question_nodes_and_xpaths_batch(self):\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)",
        "mutated": [
            "def get_question_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)",
            "def get_question_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)",
            "def get_question_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)",
            "def get_question_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)",
            "def get_question_nodes_and_xpaths_batch(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    questions = [\"what's his name?\", 'how is he called?']\n    nodes = [['hello world', 'running'], ['hello my name is bob']]\n    xpaths = [['/html/body/div/li[1]/div/span', '/html/body/div/li[1]/div/span'], ['/html/body/div/li[2]/div/span']]\n    return (questions, nodes, xpaths)"
        ]
    },
    {
        "func_name": "get_input_output_texts",
        "original": "def get_input_output_texts(self, tokenizer):\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)",
        "mutated": [
            "def get_input_output_texts(self, tokenizer):\n    if False:\n        i = 10\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)",
            "def get_input_output_texts(self, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)",
            "def get_input_output_texts(self, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)",
            "def get_input_output_texts(self, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)",
            "def get_input_output_texts(self, tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_text = 'UNwant\u00e9d,running'\n    output_text = 'unwanted, running'\n    return (input_text, output_text)"
        ]
    },
    {
        "func_name": "test_add_special_tokens",
        "original": "def test_add_special_tokens(self):\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)",
        "mutated": [
            "def test_add_special_tokens(self):\n    if False:\n        i = 10\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)",
            "def test_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)",
            "def test_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)",
            "def test_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)",
            "def test_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            special_token = '[SPECIAL_TOKEN]'\n            special_token_xpath = '/html/body/div/li[1]/div/span'\n            tokenizer.add_special_tokens({'cls_token': special_token})\n            encoded_special_token = tokenizer.encode([special_token], xpaths=[special_token_xpath], add_special_tokens=False)\n            self.assertEqual(len(encoded_special_token), 1)\n            decoded = tokenizer.decode(encoded_special_token, skip_special_tokens=True)\n            self.assertTrue(special_token not in decoded)"
        ]
    },
    {
        "func_name": "test_add_tokens_tokenizer",
        "original": "def test_add_tokens_tokenizer(self):\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)",
        "mutated": [
            "def test_add_tokens_tokenizer(self):\n    if False:\n        i = 10\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)",
            "def test_add_tokens_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)",
            "def test_add_tokens_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)",
            "def test_add_tokens_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)",
            "def test_add_tokens_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers: List[MarkupLMTokenizer] = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            vocab_size = tokenizer.vocab_size\n            all_size = len(tokenizer)\n            self.assertNotEqual(vocab_size, 0)\n            new_toks = [AddedToken('aaaaa', rstrip=True, lstrip=True), AddedToken('bbbbbb', rstrip=True, lstrip=True), AddedToken('cccccccccdddddddd', rstrip=True, lstrip=True)]\n            added_toks = tokenizer.add_tokens(new_toks)\n            vocab_size_2 = tokenizer.vocab_size\n            all_size_2 = len(tokenizer)\n            self.assertNotEqual(vocab_size_2, 0)\n            self.assertEqual(vocab_size + 3, vocab_size_2 + 3)\n            self.assertEqual(added_toks, len(new_toks))\n            self.assertEqual(all_size_2, all_size + len(new_toks))\n            nodes = 'aaaaa bbbbbb low cccccccccdddddddd l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 4)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            new_toks_2 = {'eos_token': '>>>>|||<||<<|<<', 'pad_token': '<<<<<|||>|>>>>|>'}\n            added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n            vocab_size_3 = tokenizer.vocab_size\n            all_size_3 = len(tokenizer)\n            self.assertNotEqual(vocab_size_3, 0)\n            self.assertEqual(vocab_size, vocab_size_3)\n            self.assertEqual(added_toks_2, len(new_toks_2))\n            self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n            nodes = '>>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd <<<<<|||>|>>>>|> l'.split()\n            xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n            tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertGreaterEqual(len(tokens), 6)\n            self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[0], tokens[1])\n            self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n            self.assertGreater(tokens[-2], tokens[-3])\n            self.assertEqual(tokens[0], tokenizer.eos_token_id)\n            self.assertEqual(tokens[-2], tokenizer.pad_token_id)"
        ]
    },
    {
        "func_name": "test_encode_decode_with_spaces",
        "original": "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])",
        "mutated": [
            "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])",
            "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])",
            "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])",
            "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])",
            "@require_tokenizers\ndef test_encode_decode_with_spaces(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            new_toks = [AddedToken('[ABC]', normalized=False), AddedToken('[DEF]', normalized=False)]\n            tokenizer.add_tokens(new_toks)\n            input = '[ABC][DEF][ABC][DEF]'\n            if self.space_between_special_tokens:\n                output = '[ABC] [DEF] [ABC] [DEF]'\n            else:\n                output = input\n            encoded = tokenizer.encode(input.split(), xpaths=xpaths, add_special_tokens=False)\n            decoded = tokenizer.decode(encoded, spaces_between_special_tokens=self.space_between_special_tokens)\n            self.assertIn(decoded, [output, output.lower()])"
        ]
    },
    {
        "func_name": "test_right_and_left_truncation",
        "original": "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    pass",
        "mutated": [
            "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('Not implemented')\ndef test_right_and_left_truncation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_encode_plus_with_padding",
        "original": "def test_encode_plus_with_padding(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)",
        "mutated": [
            "def test_encode_plus_with_padding(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)",
            "def test_encode_plus_with_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)",
            "def test_encode_plus_with_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)",
            "def test_encode_plus_with_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)",
            "def test_encode_plus_with_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_size = 10\n            padding_idx = tokenizer.pad_token_id\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_special_tokens_mask=True)\n            input_ids = encoded_sequence['input_ids']\n            special_tokens_mask = encoded_sequence['special_tokens_mask']\n            sequence_length = len(input_ids)\n            tokenizer.padding_side = 'right'\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            not_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, padding=False, return_special_tokens_mask=True)\n            not_padded_input_ids = not_padded_sequence['input_ids']\n            not_padded_special_tokens_mask = not_padded_sequence['special_tokens_mask']\n            not_padded_sequence_length = len(not_padded_input_ids)\n            self.assertTrue(sequence_length == not_padded_sequence_length)\n            self.assertTrue(input_ids == not_padded_input_ids)\n            self.assertTrue(special_tokens_mask == not_padded_special_tokens_mask)\n            tokenizer.padding_side = 'right'\n            right_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            right_padded_input_ids = right_padded_sequence['input_ids']\n            right_padded_special_tokens_mask = right_padded_sequence['special_tokens_mask']\n            right_padded_sequence_length = len(right_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == right_padded_sequence_length)\n            self.assertTrue(input_ids + [padding_idx] * padding_size == right_padded_input_ids)\n            self.assertTrue(special_tokens_mask + [1] * padding_size == right_padded_special_tokens_mask)\n            tokenizer.padding_side = 'left'\n            left_padded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length', return_special_tokens_mask=True)\n            left_padded_input_ids = left_padded_sequence['input_ids']\n            left_padded_special_tokens_mask = left_padded_sequence['special_tokens_mask']\n            left_padded_sequence_length = len(left_padded_input_ids)\n            self.assertTrue(sequence_length + padding_size == left_padded_sequence_length)\n            self.assertTrue([padding_idx] * padding_size + input_ids == left_padded_input_ids)\n            self.assertTrue([1] * padding_size + special_tokens_mask == left_padded_special_tokens_mask)\n            if 'token_type_ids' in tokenizer.model_input_names:\n                token_type_ids = encoded_sequence['token_type_ids']\n                left_padded_token_type_ids = left_padded_sequence['token_type_ids']\n                right_padded_token_type_ids = right_padded_sequence['token_type_ids']\n                assert token_type_ids + [0] * padding_size == right_padded_token_type_ids\n                assert [0] * padding_size + token_type_ids == left_padded_token_type_ids\n            if 'attention_mask' in tokenizer.model_input_names:\n                attention_mask = encoded_sequence['attention_mask']\n                right_padded_attention_mask = right_padded_sequence['attention_mask']\n                left_padded_attention_mask = left_padded_sequence['attention_mask']\n                self.assertTrue(attention_mask + [0] * padding_size == right_padded_attention_mask)\n                self.assertTrue([0] * padding_size + attention_mask == left_padded_attention_mask)"
        ]
    },
    {
        "func_name": "test_internal_consistency",
        "original": "def test_internal_consistency(self):\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)",
        "mutated": [
            "def test_internal_consistency(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)",
            "def test_internal_consistency(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)",
            "def test_internal_consistency(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)",
            "def test_internal_consistency(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)",
            "def test_internal_consistency(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = []\n            for word in nodes:\n                tokens.extend(tokenizer.tokenize(word))\n            ids = tokenizer.convert_tokens_to_ids(tokens)\n            ids_2 = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            self.assertListEqual(ids, ids_2)\n            tokens_2 = tokenizer.convert_ids_to_tokens(ids)\n            self.assertNotEqual(len(tokens_2), 0)\n            text_2 = tokenizer.decode(ids)\n            self.assertIsInstance(text_2, str)"
        ]
    },
    {
        "func_name": "test_mask_output",
        "original": "def test_mask_output(self):\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))",
        "mutated": [
            "def test_mask_output(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))",
            "def test_mask_output(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))",
            "def test_mask_output(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))",
            "def test_mask_output(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))",
            "def test_mask_output(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(fast=False, do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            if tokenizer.build_inputs_with_special_tokens.__qualname__.split('.')[0] != 'PreTrainedTokenizer' and 'token_type_ids' in tokenizer.model_input_names:\n                information = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                (sequences, mask) = (information['input_ids'], information['token_type_ids'])\n                self.assertEqual(len(sequences), len(mask))"
        ]
    },
    {
        "func_name": "test_number_of_added_tokens",
        "original": "def test_number_of_added_tokens(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))",
        "mutated": [
            "def test_number_of_added_tokens(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))",
            "def test_number_of_added_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))",
            "def test_number_of_added_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))",
            "def test_number_of_added_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))",
            "def test_number_of_added_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=False), len(attached_sequences) - len(sequences))\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=False)\n            attached_sequences = tokenizer.encode(question, nodes, xpaths=xpaths, add_special_tokens=True)\n            if len(attached_sequences) != 2:\n                self.assertEqual(tokenizer.num_special_tokens_to_add(pair=True), len(attached_sequences) - len(sequences))"
        ]
    },
    {
        "func_name": "test_padding_to_max_length",
        "original": "def test_padding_to_max_length(self):\n    \"\"\"We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated\"\"\"\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right",
        "mutated": [
            "def test_padding_to_max_length(self):\n    if False:\n        i = 10\n    'We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated'\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right",
            "def test_padding_to_max_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated'\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right",
            "def test_padding_to_max_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated'\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right",
            "def test_padding_to_max_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated'\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right",
            "def test_padding_to_max_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'We keep this test for backward compatibility but it should be removed when `pad_to_max_length` will be deprecated'\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, pad_to_max_length=True)\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, pad_to_max_length=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right"
        ]
    },
    {
        "func_name": "test_padding",
        "original": "def test_padding(self, max_length=50):\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)",
        "mutated": [
            "def test_padding(self, max_length=50):\n    if False:\n        i = 10\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)",
            "def test_padding(self, max_length=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)",
            "def test_padding(self, max_length=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)",
            "def test_padding(self, max_length=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)",
            "def test_padding(self, max_length=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.encode(question, nodes, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.encode(question, nodes, xpaths=xpaths, padding='longest')\n            self.assert_padded_input_match(input_r, input_p, len(input_r), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            input_r = tokenizer_r.encode_plus(question, nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.encode_plus(question, nodes, xpaths=xpaths, padding=True)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            self.assertSequenceEqual(input_r['attention_mask'], input_p['attention_mask'])\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, pad_to_max_length=True)\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, max_length=max_length, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, padding='longest')\n            input_p = tokenizer_p.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, max_length=max_length, truncation=True, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)\n            input_r = tokenizer_r.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding=True)\n            input_p = tokenizer_p.batch_encode_plus(list(zip(questions, nodes)), is_pair=True, xpaths=xpaths, padding='longest')\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], len(input_r['input_ids']), pad_token_id)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_padded_input_match(input_r['input_ids'], input_p['input_ids'], max_length, pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p)\n            self.assert_batch_padded_input_match(input_r, input_p, len(input_r['input_ids'][0]), pad_token_id)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.pad(input_r, max_length=max_length, padding='max_length')\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.pad(input_p, max_length=max_length, padding='max_length')\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id)"
        ]
    },
    {
        "func_name": "test_call",
        "original": "def test_call(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)",
        "mutated": [
            "def test_call(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)",
            "def test_call(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)",
            "def test_call(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)",
            "def test_call(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)",
            "def test_call(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            encoded_sequences_1 = tokenizer.encode_plus(nodes, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths)\n            encoded_sequences_2 = tokenizer(nodes, xpaths=xpaths)\n            self.assertEqual(encoded_sequences_1, encoded_sequences_2)"
        ]
    },
    {
        "func_name": "test_batch_encode_plus_batch_sequence_length",
        "original": "def test_batch_encode_plus_batch_sequence_length(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])",
        "mutated": [
            "def test_batch_encode_plus_batch_sequence_length(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])",
            "def test_batch_encode_plus_batch_sequence_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])",
            "def test_batch_encode_plus_batch_sequence_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])",
            "def test_batch_encode_plus_batch_sequence_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])",
            "def test_batch_encode_plus_batch_sequence_length(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example) for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n            maximum_length = len(max([encoded_sequence['input_ids'] for encoded_sequence in encoded_sequences], key=len))\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences_padded = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=maximum_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch_padded = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            self.assertListEqual(encoded_sequences_padded, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch_padded))\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=True)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding='longest')\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])\n            encoded_sequences_batch_padded_1 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, padding=False)\n            encoded_sequences_batch_padded_2 = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=maximum_length + 10, padding=False)\n            for key in encoded_sequences_batch_padded_1.keys():\n                self.assertListEqual(encoded_sequences_batch_padded_1[key], encoded_sequences_batch_padded_2[key])"
        ]
    },
    {
        "func_name": "test_batch_encode_plus_overflowing_tokens",
        "original": "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    pass",
        "mutated": [
            "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('batch_encode_plus does not handle overflowing tokens.')\ndef test_batch_encode_plus_overflowing_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_batch_encode_plus_padding",
        "original": "def test_batch_encode_plus_padding(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))",
        "mutated": [
            "def test_batch_encode_plus_padding(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))",
            "def test_batch_encode_plus_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))",
            "def test_batch_encode_plus_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))",
            "def test_batch_encode_plus_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))",
            "def test_batch_encode_plus_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            tokenizer.padding_side = 'left'\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            max_length = 100\n            self._check_no_pad_token_padding(tokenizer, nodes)\n            encoded_sequences = [tokenizer.encode_plus(nodes_example, xpaths=xpaths_example, max_length=max_length, padding='max_length') for (nodes_example, xpaths_example) in zip(nodes, xpaths)]\n            encoded_sequences_batch = tokenizer.batch_encode_plus(nodes, is_pair=False, xpaths=xpaths, max_length=max_length, padding='max_length')\n            self.assertListEqual(encoded_sequences, self.convert_batch_encode_plus_format_to_encode_plus(encoded_sequences_batch))"
        ]
    },
    {
        "func_name": "test_padding_to_multiple_of",
        "original": "def test_padding_to_multiple_of(self):\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)",
        "mutated": [
            "def test_padding_to_multiple_of(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)",
            "def test_padding_to_multiple_of(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)",
            "def test_padding_to_multiple_of(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)",
            "def test_padding_to_multiple_of(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)",
            "def test_padding_to_multiple_of(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.pad_token is None:\n                self.skipTest('No padding token.')\n            else:\n                (nodes, xpaths) = self.get_nodes_and_xpaths()\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertNotEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                normal_tokens = tokenizer(nodes, xpaths=xpaths, padding=True, truncation=True, pad_to_multiple_of=8)\n                for (key, value) in normal_tokens.items():\n                    self.assertEqual(len(value) % 8, 0, f'BatchEncoding.{key} is not multiple of 8')\n                self.assertRaises(ValueError, tokenizer.__call__, nodes, xpaths=xpaths, padding=True, truncation=True, max_length=12, pad_to_multiple_of=8)"
        ]
    },
    {
        "func_name": "test_tokenizer_slow_store_full_signature",
        "original": "def test_tokenizer_slow_store_full_signature(self):\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)",
        "mutated": [
            "def test_tokenizer_slow_store_full_signature(self):\n    if False:\n        i = 10\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)",
            "def test_tokenizer_slow_store_full_signature(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)",
            "def test_tokenizer_slow_store_full_signature(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)",
            "def test_tokenizer_slow_store_full_signature(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)",
            "def test_tokenizer_slow_store_full_signature(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    signature = inspect.signature(self.tokenizer_class.__init__)\n    tokenizer = self.get_tokenizer()\n    for (parameter_name, parameter) in signature.parameters.items():\n        if parameter.default != inspect.Parameter.empty:\n            self.assertIn(parameter_name, tokenizer.init_kwargs)"
        ]
    },
    {
        "func_name": "test_build_inputs_with_special_tokens",
        "original": "def test_build_inputs_with_special_tokens(self):\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)",
        "mutated": [
            "def test_build_inputs_with_special_tokens(self):\n    if False:\n        i = 10\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)",
            "def test_build_inputs_with_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)",
            "def test_build_inputs_with_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)",
            "def test_build_inputs_with_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)",
            "def test_build_inputs_with_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_simple = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            input_pair = tokenizer_p.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)\n            self.assertEqual(output_p, output_r)\n            output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)\n            output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)\n            self.assertEqual(output_p, output_r)"
        ]
    },
    {
        "func_name": "test_special_tokens_mask_input_pairs",
        "original": "def test_special_tokens_mask_input_pairs(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
        "mutated": [
            "def test_special_tokens_mask_input_pairs(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask_input_pairs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask_input_pairs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask_input_pairs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask_input_pairs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x if not special_tokens_mask[i] else None for (i, x) in enumerate(encoded_sequence_w_special)]\n            filtered_sequence = [x for x in filtered_sequence if x is not None]\n            self.assertEqual(encoded_sequence, filtered_sequence)"
        ]
    },
    {
        "func_name": "test_special_tokens_mask",
        "original": "def test_special_tokens_mask(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
        "mutated": [
            "def test_special_tokens_mask(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)",
            "def test_special_tokens_mask(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            encoded_sequence_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True, return_special_tokens_mask=True)\n            encoded_sequence_w_special = encoded_sequence_dict['input_ids']\n            special_tokens_mask = encoded_sequence_dict['special_tokens_mask']\n            self.assertEqual(len(special_tokens_mask), len(encoded_sequence_w_special))\n            filtered_sequence = [x for (i, x) in enumerate(encoded_sequence_w_special) if not special_tokens_mask[i]]\n            self.assertEqual(encoded_sequence, filtered_sequence)"
        ]
    },
    {
        "func_name": "test_save_and_load_tokenizer",
        "original": "def test_save_and_load_tokenizer(self):\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)",
        "mutated": [
            "def test_save_and_load_tokenizer(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)",
            "def test_save_and_load_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)",
            "def test_save_and_load_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)",
            "def test_save_and_load_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)",
            "def test_save_and_load_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            self.assertNotEqual(tokenizer.model_max_length, 42)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tmpdirname = tempfile.mkdtemp()\n            before_tokens = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            before_vocab = tokenizer.get_vocab()\n            tokenizer.save_pretrained(tmpdirname)\n            after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n            after_tokens = after_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            after_vocab = after_tokenizer.get_vocab()\n            self.assertListEqual(before_tokens, after_tokens)\n            self.assertDictEqual(before_vocab, after_vocab)\n            shutil.rmtree(tmpdirname)"
        ]
    },
    {
        "func_name": "test_right_and_left_padding",
        "original": "def test_right_and_left_padding(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left",
        "mutated": [
            "def test_right_and_left_padding(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left",
            "def test_right_and_left_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left",
            "def test_right_and_left_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left",
            "def test_right_and_left_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left",
            "def test_right_and_left_padding(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            sequence = 'Sequence'\n            padding_size = 10\n            self._check_no_pad_token_padding(tokenizer, sequence)\n            padding_idx = tokenizer.pad_token_id\n            tokenizer.padding_side = 'right'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert encoded_sequence + [padding_idx] * padding_size == padded_sequence\n            tokenizer.padding_side = 'left'\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            padded_sequence = tokenizer.encode(nodes, xpaths=xpaths, max_length=sequence_length + padding_size, padding='max_length')\n            padded_sequence_length = len(padded_sequence)\n            assert sequence_length + padding_size == padded_sequence_length\n            assert [padding_idx] * padding_size + encoded_sequence == padded_sequence\n            encoded_sequence = tokenizer.encode(nodes, xpaths=xpaths)\n            sequence_length = len(encoded_sequence)\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths, padding=True)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding='longest')\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left\n            tokenizer.padding_side = 'right'\n            padded_sequence_right = tokenizer.encode(nodes, xpaths=xpaths)\n            padded_sequence_right_length = len(padded_sequence_right)\n            assert sequence_length == padded_sequence_right_length\n            assert encoded_sequence == padded_sequence_right\n            tokenizer.padding_side = 'left'\n            padded_sequence_left = tokenizer.encode(nodes, xpaths=xpaths, padding=False)\n            padded_sequence_left_length = len(padded_sequence_left)\n            assert sequence_length == padded_sequence_left_length\n            assert encoded_sequence == padded_sequence_left"
        ]
    },
    {
        "func_name": "test_token_type_ids",
        "original": "def test_token_type_ids(self):\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])",
        "mutated": [
            "def test_token_type_ids(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])",
            "def test_token_type_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])",
            "def test_token_type_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])",
            "def test_token_type_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])",
            "def test_token_type_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            output = tokenizer(nodes, xpaths=xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])\n            self.assertNotIn(1, output['token_type_ids'])\n            (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n            output = tokenizer(question, nodes, xpaths, return_token_type_ids=True)\n            self.assertEqual(len(output['token_type_ids']), len(output['input_ids']))\n            self.assertEqual(len(output['token_type_ids']), len(output['attention_mask']))\n            self.assertIn(0, output['token_type_ids'])"
        ]
    },
    {
        "func_name": "test_offsets_mapping",
        "original": "def test_offsets_mapping(self):\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)",
        "mutated": [
            "def test_offsets_mapping(self):\n    if False:\n        i = 10\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)",
            "def test_offsets_mapping(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)",
            "def test_offsets_mapping(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)",
            "def test_offsets_mapping(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)",
            "def test_offsets_mapping(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            text = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(text))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(False)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)\n            text = \"what's his name\"\n            pair = ['a', 'wonderful', 'test']\n            xpaths = ['html/body' for _ in range(len(pair))]\n            tokens_with_offsets = tokenizer_r.encode_plus(text, pair, xpaths=xpaths, return_special_tokens_mask=True, return_offsets_mapping=True, add_special_tokens=True)\n            added_tokens = tokenizer_r.num_special_tokens_to_add(True)\n            offsets = tokens_with_offsets['offset_mapping']\n            self.assertEqual(len(offsets), len(tokens_with_offsets['input_ids']))\n            self.assertEqual(sum(tokens_with_offsets['special_tokens_mask']), added_tokens)"
        ]
    },
    {
        "func_name": "test_torch_encode_plus_sent_to_model",
        "original": "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)",
        "mutated": [
            "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)",
            "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)",
            "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)",
            "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)",
            "@require_torch\n@slow\ndef test_torch_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import torch\n    from transformers import MODEL_MAPPING, TOKENIZER_MAPPING\n    MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(MODEL_MAPPING, TOKENIZER_MAPPING)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n                return\n            (config_class, model_class) = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n            config = config_class()\n            if config.is_encoder_decoder or config.pad_token_id is None:\n                return\n            model = model_class(config)\n            is_using_common_embeddings = hasattr(model.get_input_embeddings(), 'weight')\n            assert model.get_input_embeddings().weight.shape[0] >= len(tokenizer) if is_using_common_embeddings else True\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            encoded_sequence = tokenizer.encode_plus(nodes, xpaths=xpaths, return_tensors='pt')\n            batch_encoded_sequence = tokenizer.batch_encode_plus([nodes, nodes], [xpaths, xpaths], return_tensors='pt')\n            with torch.no_grad():\n                model(**encoded_sequence)\n                model(**batch_encoded_sequence)"
        ]
    },
    {
        "func_name": "test_rust_and_python_full_tokenizers",
        "original": "def test_rust_and_python_full_tokenizers(self):\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)",
        "mutated": [
            "def test_rust_and_python_full_tokenizers(self):\n    if False:\n        i = 10\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)",
            "def test_rust_and_python_full_tokenizers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)",
            "def test_rust_and_python_full_tokenizers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)",
            "def test_rust_and_python_full_tokenizers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)",
            "def test_rust_and_python_full_tokenizers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_rust_tokenizer:\n        return\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer = self.get_tokenizer()\n    rust_tokenizer = self.get_rust_tokenizer()\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    self.assertListEqual(ids, rust_ids)\n    ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    rust_ids = rust_tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n    self.assertListEqual(ids, rust_ids)"
        ]
    },
    {
        "func_name": "test_tokenization_python_rust_equals",
        "original": "def test_tokenization_python_rust_equals(self):\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])",
        "mutated": [
            "def test_tokenization_python_rust_equals(self):\n    if False:\n        i = 10\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])",
            "def test_tokenization_python_rust_equals(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])",
            "def test_tokenization_python_rust_equals(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])",
            "def test_tokenization_python_rust_equals(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])",
            "def test_tokenization_python_rust_equals(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_pairs_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths)\n            input_pairs_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_pairs_p[key], input_pairs_r[key])\n            nodes = ['hello' for _ in range(1000)]\n            xpaths = ['html/body' for _ in range(1000)]\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key])\n            input_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            input_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, max_length=512, truncation=True, stride=3, return_overflowing_tokens=True)\n            for key in filter(lambda x: x in ['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'], input_p.keys()):\n                self.assertSequenceEqual(input_p[key], input_r[key][0])"
        ]
    },
    {
        "func_name": "test_embeded_special_tokens",
        "original": "def test_embeded_special_tokens(self):\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)",
        "mutated": [
            "def test_embeded_special_tokens(self):\n    if False:\n        i = 10\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)",
            "def test_embeded_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)",
            "def test_embeded_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)",
            "def test_embeded_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)",
            "def test_embeded_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in tokens_p.keys():\n                self.assertEqual(tokens_r[key], tokens_p[key])\n            if 'token_type_ids' in tokens_r:\n                self.assertEqual(sum(tokens_r['token_type_ids']), sum(tokens_p['token_type_ids']))\n            tokens_r = tokenizer_r.convert_ids_to_tokens(tokens_r['input_ids'])\n            tokens_p = tokenizer_p.convert_ids_to_tokens(tokens_p['input_ids'])\n            self.assertSequenceEqual(tokens_r, tokens_p)"
        ]
    },
    {
        "func_name": "test_compare_add_special_tokens",
        "original": "def test_compare_add_special_tokens(self):\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)",
        "mutated": [
            "def test_compare_add_special_tokens(self):\n    if False:\n        i = 10\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)",
            "def test_compare_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)",
            "def test_compare_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)",
            "def test_compare_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)",
            "def test_compare_add_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            no_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=False)\n            with_special_tokens = tokenizer_r.tokenize(' '.join(nodes), add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(len(no_special_tokens), len(with_special_tokens) - simple_num_special_tokens_to_add)\n            no_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                self.assertEqual(len(no_special_tokens[key]), len(with_special_tokens[key]) - simple_num_special_tokens_to_add)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            no_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=False)\n            with_special_tokens = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            for key in no_special_tokens.keys():\n                for (i_no, i_with) in zip(no_special_tokens[key], with_special_tokens[key]):\n                    self.assertEqual(len(i_no), len(i_with) - simple_num_special_tokens_to_add)"
        ]
    },
    {
        "func_name": "test_markuplm_truncation_integration_test",
        "original": "@slow\ndef test_markuplm_truncation_integration_test(self):\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)",
        "mutated": [
            "@slow\ndef test_markuplm_truncation_integration_test(self):\n    if False:\n        i = 10\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)",
            "@slow\ndef test_markuplm_truncation_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)",
            "@slow\ndef test_markuplm_truncation_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)",
            "@slow\ndef test_markuplm_truncation_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)",
            "@slow\ndef test_markuplm_truncation_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', model_max_length=512)\n    for i in range(12, 512):\n        new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, max_length=i, truncation=True)\n        self.assertLessEqual(len(new_encoded_inputs), i)\n    tokenizer.model_max_length = 20\n    new_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    dropped_encoded_inputs = tokenizer.encode(nodes, xpaths=xpaths, truncation=True)\n    self.assertListEqual(new_encoded_inputs, dropped_encoded_inputs)\n    self.assertLessEqual(len(new_encoded_inputs), 20)"
        ]
    },
    {
        "func_name": "test_batch_encode_plus_tensors",
        "original": "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)",
        "mutated": [
            "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)",
            "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)",
            "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)",
            "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)",
            "@is_pt_tf_cross_test\ndef test_batch_encode_plus_tensors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='pt')\n            self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, return_tensors='tf')\n            if tokenizer.pad_token_id is None:\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                self.assertRaises(ValueError, tokenizer.batch_encode_plus, nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n            else:\n                pytorch_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True, return_tensors='pt')\n                tensorflow_tensor = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding='longest', return_tensors='tf')\n                encoded_sequences = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, padding=True)\n                for key in encoded_sequences.keys():\n                    pytorch_value = pytorch_tensor[key].tolist()\n                    tensorflow_value = tensorflow_tensor[key].numpy().tolist()\n                    encoded_value = encoded_sequences[key]\n                    self.assertEqual(pytorch_value, tensorflow_value, encoded_value)"
        ]
    },
    {
        "func_name": "test_sequence_ids",
        "original": "def test_sequence_ids(self):\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())",
        "mutated": [
            "def test_sequence_ids(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())",
            "def test_sequence_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())",
            "def test_sequence_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())",
            "def test_sequence_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())",
            "def test_sequence_ids(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers()\n    for tokenizer in tokenizers:\n        if not tokenizer.is_fast:\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            seq_0 = 'Test this method.'\n            seq_1 = ['With', 'these', 'inputs.']\n            xpaths = ['html/body' for _ in range(len(seq_1))]\n            output = tokenizer(seq_0.split(), xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            output = tokenizer(seq_0, seq_1, xpaths=xpaths)\n            self.assertIn(0, output.sequence_ids())\n            self.assertIn(1, output.sequence_ids())\n            if tokenizer.num_special_tokens_to_add(pair=True):\n                self.assertIn(None, output.sequence_ids())"
        ]
    },
    {
        "func_name": "test_special_tokens_initialization",
        "original": "def test_special_tokens_initialization(self):\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)",
        "mutated": [
            "def test_special_tokens_initialization(self):\n    if False:\n        i = 10\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)",
            "def test_special_tokens_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)",
            "def test_special_tokens_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)",
            "def test_special_tokens_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)",
            "def test_special_tokens_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            added_tokens = [AddedToken('<special>', lstrip=True)]\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n            nodes = 'Hey this is a <special> token'.split()\n            xpaths = ['html/body' for _ in range(len(nodes))]\n            r_output = tokenizer_r.encode(nodes, xpaths=xpaths)\n            special_token_id = tokenizer_r.encode(['<special>'], xpaths=['html/body'], add_special_tokens=False)[0]\n            self.assertTrue(special_token_id in r_output)\n            if self.test_slow_tokenizer:\n                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n                nodes = 'Hey this is a <special> token'.split()\n                xpaths = ['html/body' for _ in range(len(nodes))]\n                p_output = tokenizer_p.encode(nodes, xpaths=xpaths)\n                cr_output = tokenizer_cr.encode(nodes, xpaths=xpaths)\n                self.assertEqual(p_output, r_output)\n                self.assertEqual(cr_output, r_output)\n                self.assertTrue(special_token_id in p_output)\n                self.assertTrue(special_token_id in cr_output)"
        ]
    },
    {
        "func_name": "test_split_special_tokens",
        "original": "def test_split_special_tokens(self):\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)",
        "mutated": [
            "def test_split_special_tokens(self):\n    if False:\n        i = 10\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)",
            "def test_split_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)",
            "def test_split_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)",
            "def test_split_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)",
            "def test_split_special_tokens(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizer = self.get_tokenizer()\n    special_token = '[SPECIAL_TOKEN]'\n    tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n    encoded_special_token = tokenizer.tokenize(special_token, add_special_tokens=False)\n    self.assertEqual(len(encoded_special_token), 1)\n    encoded_split_special_token = tokenizer.tokenize(special_token, add_special_tokens=False, split_special_tokens=True)\n    self.assertTrue(len(encoded_split_special_token) > 1)"
        ]
    },
    {
        "func_name": "test_training_new_tokenizer",
        "original": "def test_training_new_tokenizer(self):\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)",
        "mutated": [
            "def test_training_new_tokenizer(self):\n    if False:\n        i = 10\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)",
            "def test_training_new_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)",
            "def test_training_new_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)",
            "def test_training_new_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)",
            "def test_training_new_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100)\n    text = [['this', 'is', 'the'], ['how', 'are', 'you']]\n    xpaths = [['html/body'] * 3, ['html/body'] * 3]\n    inputs = new_tokenizer(text, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisisthe'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)\n    self.assertEqual(tokenizer.num_special_tokens_to_add(False), new_tokenizer.num_special_tokens_to_add(False))\n    self.assertEqual(tokenizer.num_special_tokens_to_add(True), new_tokenizer.num_special_tokens_to_add(True))\n    self.assertEqual(tokenizer.max_len_single_sentence, new_tokenizer.max_len_single_sentence)\n    self.assertEqual(tokenizer.max_len_sentences_pair, new_tokenizer.max_len_sentences_pair)\n    self.assertSequenceEqual(tokenizer.all_special_tokens_extended, new_tokenizer.all_special_tokens_extended)\n    self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)"
        ]
    },
    {
        "func_name": "test_training_new_tokenizer_with_special_tokens_change",
        "original": "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)",
        "mutated": [
            "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if False:\n        i = 10\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)",
            "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)",
            "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)",
            "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)",
            "def test_training_new_tokenizer_with_special_tokens_change(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_rust_tokenizer:\n        return\n    tokenizer = self.get_rust_tokenizer()\n    class_signature = inspect.signature(tokenizer.__class__)\n    if 'cls_token' in class_signature.parameters:\n        new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map={tokenizer.cls_token: '<cls>'})\n        cls_id = new_tokenizer.get_vocab()['<cls>']\n        self.assertEqual(new_tokenizer.cls_token, '<cls>')\n        self.assertEqual(new_tokenizer.cls_token_id, cls_id)\n    special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n    special_tokens_list.remove('additional_special_tokens')\n    special_tokens_map = {}\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is not None:\n            special_token = getattr(tokenizer, token)\n            special_tokens_map[special_token] = f'{special_token}a'\n    new_tokenizer = tokenizer.train_new_from_iterator(SMALL_TRAINING_CORPUS, 100, special_tokens_map=special_tokens_map)\n    for token in special_tokens_list:\n        if getattr(tokenizer, f'_{token}') is None:\n            continue\n        special_token = getattr(tokenizer, token)\n        if special_token in special_tokens_map:\n            new_special_token = getattr(new_tokenizer, token)\n            self.assertEqual(special_tokens_map[special_token], new_special_token)\n            new_id = new_tokenizer.get_vocab()[new_special_token]\n            self.assertEqual(getattr(new_tokenizer, f'{token}_id'), new_id)\n    for special_token in tokenizer.all_special_tokens_extended:\n        if isinstance(special_token, AddedToken) and special_token.content not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        elif isinstance(special_token, AddedToken):\n            special_token_str = special_token.content\n            new_special_token_str = special_tokens_map[special_token_str]\n            find = False\n            for candidate in new_tokenizer.all_special_tokens_extended:\n                if isinstance(candidate, AddedToken) and candidate.content == new_special_token_str and (candidate.lstrip == special_token.lstrip) and (candidate.rstrip == special_token.rstrip) and (candidate.normalized == special_token.normalized) and (candidate.single_word == special_token.single_word):\n                    find = True\n                    break\n            self.assertTrue(find, f\"'{new_special_token_str}' doesn't appear in the list '{new_tokenizer.all_special_tokens_extended}' as an AddedToken with the same parameters as '{special_token}' in the list {tokenizer.all_special_tokens_extended}\")\n        elif special_token not in special_tokens_map:\n            self.assertTrue(special_token in new_tokenizer.all_special_tokens_extended, f\"'{special_token}' should be in {new_tokenizer.all_special_tokens_extended}\")\n        else:\n            self.assertTrue(special_tokens_map[special_token] in new_tokenizer.all_special_tokens_extended)\n    nodes = [['this', 'is'], ['hello', '\ud83e\udd17']]\n    xpaths = [['html/body'] * 2, ['html/body'] * 2]\n    inputs = new_tokenizer(nodes, xpaths=xpaths)\n    self.assertEqual(len(inputs['input_ids']), 2)\n    decoded_input = new_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n    expected_result = 'thisis'\n    if tokenizer.backend_tokenizer.normalizer is not None:\n        expected_result = tokenizer.backend_tokenizer.normalizer.normalize_str(expected_result)\n    self.assertEqual(expected_result, decoded_input)"
        ]
    },
    {
        "func_name": "test_prepare_for_model",
        "original": "def test_prepare_for_model(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)",
        "mutated": [
            "def test_prepare_for_model(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)",
            "def test_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)",
            "def test_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)",
            "def test_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)",
            "def test_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False)\n    for tokenizer in tokenizers:\n        if tokenizer.__class__.__name__ == 'MarkupLMTokenizerFast':\n            continue\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            prepared_input_dict = tokenizer.prepare_for_model(nodes, xpaths=xpaths, add_special_tokens=True)\n            input_dict = tokenizer.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n            self.assertEqual(input_dict, prepared_input_dict)"
        ]
    },
    {
        "func_name": "test_padding_different_model_input_name",
        "original": "def test_padding_different_model_input_name(self):\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')",
        "mutated": [
            "def test_padding_different_model_input_name(self):\n    if False:\n        i = 10\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')",
            "def test_padding_different_model_input_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')",
            "def test_padding_different_model_input_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')",
            "def test_padding_different_model_input_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')",
            "def test_padding_different_model_input_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_slow_tokenizer:\n        return\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name})'):\n            tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n            self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n            pad_token_id = tokenizer_p.pad_token_id\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            input_r = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_p = tokenizer_r.batch_encode_plus(nodes, xpaths=xpaths)\n            input_r['inputs'] = input_r[tokenizer_r.model_input_names[0]]\n            del input_r[tokenizer_r.model_input_names[0]]\n            input_p['inputs'] = input_p[tokenizer_p.model_input_names[0]]\n            del input_p[tokenizer_p.model_input_names[0]]\n            tokenizer_r.model_input_names = ['inputs'] + tokenizer_r.model_input_names[1:]\n            tokenizer_p.model_input_names = ['inputs'] + tokenizer_p.model_input_names[1:]\n            input_r = tokenizer_r.pad(input_r, padding='longest')\n            input_p = tokenizer_r.pad(input_p, padding='longest')\n            max_length = len(input_p['inputs'][0])\n            self.assert_batch_padded_input_match(input_r, input_p, max_length, pad_token_id, model_main_input_name='inputs')"
        ]
    },
    {
        "func_name": "test_batch_encode_dynamic_overflowing",
        "original": "def test_batch_encode_dynamic_overflowing(self):\n    \"\"\"\n        When calling batch_encode with multiple sequences, it can return different number of\n        overflowing encoding for each sequence:\n        [\n          Sequence 1: [Encoding 1, Encoding 2],\n          Sequence 2: [Encoding 1],\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\n        ]\n        This needs to be padded so that it can represented as a tensor\n        \"\"\"\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)",
        "mutated": [
            "def test_batch_encode_dynamic_overflowing(self):\n    if False:\n        i = 10\n    '\\n        When calling batch_encode with multiple sequences, it can return different number of\\n        overflowing encoding for each sequence:\\n        [\\n          Sequence 1: [Encoding 1, Encoding 2],\\n          Sequence 2: [Encoding 1],\\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\\n        ]\\n        This needs to be padded so that it can represented as a tensor\\n        '\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)",
            "def test_batch_encode_dynamic_overflowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        When calling batch_encode with multiple sequences, it can return different number of\\n        overflowing encoding for each sequence:\\n        [\\n          Sequence 1: [Encoding 1, Encoding 2],\\n          Sequence 2: [Encoding 1],\\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\\n        ]\\n        This needs to be padded so that it can represented as a tensor\\n        '\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)",
            "def test_batch_encode_dynamic_overflowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        When calling batch_encode with multiple sequences, it can return different number of\\n        overflowing encoding for each sequence:\\n        [\\n          Sequence 1: [Encoding 1, Encoding 2],\\n          Sequence 2: [Encoding 1],\\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\\n        ]\\n        This needs to be padded so that it can represented as a tensor\\n        '\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)",
            "def test_batch_encode_dynamic_overflowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        When calling batch_encode with multiple sequences, it can return different number of\\n        overflowing encoding for each sequence:\\n        [\\n          Sequence 1: [Encoding 1, Encoding 2],\\n          Sequence 2: [Encoding 1],\\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\\n        ]\\n        This needs to be padded so that it can represented as a tensor\\n        '\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)",
            "def test_batch_encode_dynamic_overflowing(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        When calling batch_encode with multiple sequences, it can return different number of\\n        overflowing encoding for each sequence:\\n        [\\n          Sequence 1: [Encoding 1, Encoding 2],\\n          Sequence 2: [Encoding 1],\\n          Sequence 3: [Encoding 1, Encoding 2, ... Encoding N]\\n        ]\\n        This needs to be padded so that it can represented as a tensor\\n        '\n    for (tokenizer, pretrained_name, kwargs) in self.tokenizers_list:\n        tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n        with self.subTest(f'{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})'):\n            if is_torch_available():\n                returned_tensor = 'pt'\n            elif is_tf_available():\n                returned_tensor = 'tf'\n            else:\n                returned_tensor = 'jax'\n            (nodes, xpaths) = self.get_nodes_and_xpaths()\n            tokens = tokenizer.encode_plus(nodes, xpaths=xpaths, max_length=1, padding=True, truncation=True, return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n            (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n            tokens = tokenizer.batch_encode_plus(nodes, xpaths=xpaths, max_length=6, padding=True, truncation='only_first', return_tensors=returned_tensor, return_overflowing_tokens=True)\n            for key in filter(lambda x: 'overflow_to_sample_mapping' not in x, tokens.keys()):\n                if 'xpath' not in key:\n                    self.assertEqual(len(tokens[key].shape), 2)\n                    self.assertEqual(tokens[key].shape[-1], 6)\n                else:\n                    self.assertEqual(len(tokens[key].shape), 3)\n                    self.assertEqual(tokens[key].shape[-2], 6)"
        ]
    },
    {
        "func_name": "test_alignement_methods",
        "original": "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    pass",
        "mutated": [
            "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('TO DO: overwrite this very extensive test.')\ndef test_alignement_methods(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "get_clean_sequence",
        "original": "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)",
        "mutated": [
            "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    if False:\n        i = 10\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)",
            "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)",
            "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)",
            "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)",
            "def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    toks = [(i, tokenizer.decode([i], clean_up_tokenization_spaces=False)) for i in range(len(tokenizer))]\n    toks = list(filter(lambda t: re.match('^[ a-zA-Z]+$', t[1]), toks))\n    toks = list(filter(lambda t: [t[0]] == tokenizer.encode(t[1].split(' '), xpaths=len(t[1]) * ['html/body'], add_special_tokens=False), toks))\n    if max_length is not None and len(toks) > max_length:\n        toks = toks[:max_length]\n    if min_length is not None and len(toks) < min_length and (len(toks) > 0):\n        while len(toks) < min_length:\n            toks = toks + toks\n    toks_ids = [t[0] for t in toks]\n    output_txt = tokenizer.decode(toks_ids, clean_up_tokenization_spaces=False)\n    '\\n        if \" \" not in output_txt and len(toks_ids) > 1:\\n            output_txt = (\\n                tokenizer.decode([toks_ids[0]], clean_up_tokenization_spaces=False)\\n                + \" \"\\n                + tokenizer.decode(toks_ids[1:], clean_up_tokenization_spaces=False)\\n            )\\n        '\n    if with_prefix_space:\n        output_txt = ' ' + output_txt\n    nodes = output_txt.split(' ')\n    xpaths = ['html/body' for i in range(len(nodes))]\n    output_ids = tokenizer.encode(nodes, xpaths=xpaths, add_special_tokens=False)\n    return (nodes, xpaths, output_ids)"
        ]
    },
    {
        "func_name": "test_maximum_encoding_length_pair_input",
        "original": "def test_maximum_encoding_length_pair_input(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)",
        "mutated": [
            "def test_maximum_encoding_length_pair_input(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)",
            "def test_maximum_encoding_length_pair_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)",
            "def test_maximum_encoding_length_pair_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)",
            "def test_maximum_encoding_length_pair_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)",
            "def test_maximum_encoding_length_pair_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            stride = 2\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            question_0 = ' '.join(map(str, seq_0))\n            if len(ids) <= 2 + stride:\n                seq_0 = (seq_0 + ' ') * (2 + stride)\n                ids = None\n            seq0_tokens = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            self.assertGreater(len(seq0_tokens['input_ids']), 2 + stride)\n            question_1 = 'This is another sentence to be encoded.'\n            seq_1 = ['hello', 'world']\n            xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            if abs(len(seq0_tokens['input_ids']) - len(seq1_tokens['input_ids'])) <= 2:\n                seq1_tokens_input_ids = seq1_tokens['input_ids'] + seq1_tokens['input_ids']\n                seq_1 = tokenizer.decode(seq1_tokens_input_ids, clean_up_tokenization_spaces=False)\n                seq_1 = seq_1.split(' ')\n                xpaths_1 = ['html/body' for i in range(len(seq_1))]\n            seq1_tokens = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            self.assertGreater(len(seq1_tokens['input_ids']), 2 + stride)\n            smallest = seq1_tokens['input_ids'] if len(seq0_tokens['input_ids']) > len(seq1_tokens['input_ids']) else seq0_tokens['input_ids']\n            sequence = tokenizer(question_0, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_2 = seq_0 * model_max_length\n            question_2 = ' '.join(map(str, seq_2))\n            xpaths_2 = xpaths_0 * model_max_length\n            self.assertGreaterEqual(len(seq_2), model_max_length)\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            sequence2 = tokenizer(question_2, seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length2 = len(sequence2['input_ids'])\n            self.assertLess(total_length1, model_max_length, 'Issue with the testing sequence, please update it.')\n            self.assertGreater(total_length2, model_max_length, 'Issue with the testing sequence, please update it.')\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'{tokenizer.__class__.__name__} Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'{tokenizer.__class__.__name__} Truncation: {truncation_state}'):\n                            output = tokenizer(question_2, seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([question_2], [seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids']), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                    output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation='only_second')\n                    self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                    self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(question_1, seq_2, xpaths=xpaths_2, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([question_1], [seq_2], xpaths=[xpaths_2], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            truncated_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][:-2] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            truncated_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][:-2]\n            truncated_longest_sequence = truncated_first_sequence if len(seq0_tokens) > len(seq1_tokens) else truncated_second_sequence\n            overflow_first_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'][-(2 + stride):] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids']\n            overflow_second_sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'] + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['input_ids'][-(2 + stride):]\n            overflow_longest_sequence = overflow_first_sequence if len(seq0_tokens) > len(seq1_tokens) else overflow_second_sequence\n            xpath_tags_seq_first = [[5] * 50] * (len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) - 2)\n            xpath_tags_seq_first_sequence = xpath_tags_seq_first + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            overflowing_token_xpath_tags_seq_first_sequence_slow = [[5] * 50] * (2 + stride)\n            overflowing_token_xpath_tags_seq_first_sequence_fast = [[5] * 50] * (2 + stride) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq']\n            xpath_tags_seq_second = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids'])\n            xpath_tags_seq_second_sequence = xpath_tags_seq_second + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][:-2]\n            overflowing_token_xpath_tags_seq_second_sequence_slow = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            overflowing_token_xpath_tags_seq_second_sequence_fast = [[5] * 50] * len(tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)['input_ids']) + tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)['xpath_tags_seq'][-(2 + stride):]\n            xpath_tags_seq_longest_sequence = xpath_tags_seq_first_sequence if len(seq0_tokens) > len(seq1_tokens) else xpath_tags_seq_second_sequence\n            overflowing_token_xpath_tags_seq_longest_sequence_fast = overflowing_token_xpath_tags_seq_first_sequence_fast if len(seq0_tokens) > len(seq1_tokens) else overflowing_token_xpath_tags_seq_second_sequence_fast\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(len(overflowing_xpath_tags_seq), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='longest_first', return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_longest_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(smallest))\n                self.assertEqual(overflowing_tokens, overflow_longest_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_longest_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_longest_sequence_fast)\n            else:\n                with self.assertRaises(ValueError) as context:\n                    information = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n                self.assertTrue(context.exception.args[0].startswith('Not possible to return overflowing tokens for pair of sequences with the `longest_first`. Please select another truncation strategy than `longest_first`, for instance `only_second` or `only_first`.'))\n            information_first_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_first', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_first_truncated['input_ids'][0]\n                overflowing_tokens = information_first_truncated['input_ids'][1]\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_first_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_first_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq1_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_first_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_fast)\n            else:\n                truncated_sequence = information_first_truncated['input_ids']\n                overflowing_tokens = information_first_truncated['overflowing_tokens']\n                overflowing_xpath_tags_seq = information_first_truncated['overflowing_xpath_tags_seq']\n                xpath_tags_seq = information_first_truncated['xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_first_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq0_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_first_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_first_sequence_slow)\n            information_second_truncated = tokenizer(question_0, seq_1, xpaths=xpaths_1, max_length=len(sequence['input_ids']) - 2, add_special_tokens=False, stride=stride, truncation='only_second', return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information_second_truncated['input_ids'][0]\n                overflowing_tokens = information_second_truncated['input_ids'][1]\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information_second_truncated['xpath_tags_seq'][1]\n                self.assertEqual(len(information_second_truncated['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride + len(seq0_tokens['input_ids']))\n                self.assertEqual(overflowing_tokens, overflow_second_sequence)\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_fast)\n            else:\n                truncated_sequence = information_second_truncated['input_ids']\n                overflowing_tokens = information_second_truncated['overflowing_tokens']\n                xpath_tags_seq = information_second_truncated['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information_second_truncated['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), len(sequence['input_ids']) - 2)\n                self.assertEqual(truncated_sequence, truncated_second_sequence)\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, seq1_tokens['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, xpath_tags_seq_second_sequence)\n                self.assertEqual(overflowing_xpath_tags_seq, overflowing_token_xpath_tags_seq_second_sequence_slow)"
        ]
    },
    {
        "func_name": "test_maximum_encoding_length_single_input",
        "original": "def test_maximum_encoding_length_single_input(self):\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])",
        "mutated": [
            "def test_maximum_encoding_length_single_input(self):\n    if False:\n        i = 10\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])",
            "def test_maximum_encoding_length_single_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])",
            "def test_maximum_encoding_length_single_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])",
            "def test_maximum_encoding_length_single_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])",
            "def test_maximum_encoding_length_single_input(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizers = self.get_tokenizers(do_lower_case=False, model_max_length=100)\n    for tokenizer in tokenizers:\n        with self.subTest(f'{tokenizer.__class__.__name__}'):\n            (seq_0, xpaths_0, ids) = self.get_clean_sequence(tokenizer, max_length=20)\n            sequence = tokenizer(seq_0, xpaths=xpaths_0, add_special_tokens=False)\n            total_length = len(sequence['input_ids'])\n            self.assertGreater(total_length, 4, \"Issue with the testing sequence, please update it it's too short\")\n            model_max_length = tokenizer.model_max_length\n            self.assertEqual(model_max_length, 100)\n            seq_1 = seq_0 * model_max_length\n            xpaths_1 = xpaths_0 * model_max_length\n            sequence1 = tokenizer(seq_1, xpaths=xpaths_1, add_special_tokens=False)\n            total_length1 = len(sequence1['input_ids'])\n            self.assertGreater(total_length1, model_max_length, \"Issue with the testing sequence, please update it it's too short\")\n            padding_strategies = [False, True, 'longest'] if tokenizer.pad_token and tokenizer.pad_token_id >= 0 else [False]\n            for padding_state in padding_strategies:\n                with self.subTest(f'Padding: {padding_state}'):\n                    for truncation_state in [True, 'longest_first', 'only_first']:\n                        with self.subTest(f'Truncation: {truncation_state}'):\n                            output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids']), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq']), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq']), model_max_length)\n                            output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=truncation_state)\n                            self.assertEqual(len(output['input_ids'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                            self.assertEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer(seq_1, xpaths=xpaths_1, padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq']), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq']), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n                    tokenizer.deprecation_warnings = {}\n                    with self.assertLogs('transformers', level='WARNING') as cm:\n                        output = tokenizer([seq_1], xpaths=[xpaths_1], padding=padding_state, truncation=False)\n                        self.assertNotEqual(len(output['input_ids'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_tags_seq'][0]), model_max_length)\n                        self.assertNotEqual(len(output['xpath_subs_seq'][0]), model_max_length)\n                    self.assertEqual(len(cm.records), 1)\n                    self.assertTrue(cm.records[0].message.startswith('Token indices sequence length is longer than the specified maximum sequence length for this model'))\n            stride = 2\n            information = tokenizer(seq_0, xpaths=xpaths_0, max_length=total_length - 2, add_special_tokens=False, stride=stride, truncation=True, return_overflowing_tokens=True)\n            if isinstance(tokenizer, MarkupLMTokenizerFast):\n                truncated_sequence = information['input_ids'][0]\n                overflowing_tokens = information['input_ids'][1]\n                xpath_tags_seq = information['xpath_tags_seq'][0]\n                overflowing_xpath_tags_seq = information['xpath_tags_seq'][1]\n                self.assertEqual(len(information['input_ids']), 2)\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])\n            else:\n                truncated_sequence = information['input_ids']\n                overflowing_tokens = information['overflowing_tokens']\n                xpath_tags_seq = information['xpath_tags_seq']\n                overflowing_xpath_tags_seq = information['overflowing_xpath_tags_seq']\n                self.assertEqual(len(truncated_sequence), total_length - 2)\n                self.assertEqual(truncated_sequence, sequence['input_ids'][:-2])\n                self.assertEqual(len(overflowing_tokens), 2 + stride)\n                self.assertEqual(overflowing_tokens, sequence['input_ids'][-(2 + stride):])\n                self.assertEqual(xpath_tags_seq, sequence['xpath_tags_seq'][:-2])\n                self.assertEqual(overflowing_xpath_tags_seq, sequence['xpath_tags_seq'][-(2 + stride):])"
        ]
    },
    {
        "func_name": "test_pretokenized_inputs",
        "original": "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    pass",
        "mutated": [
            "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('MarkupLM tokenizer requires xpaths besides sequences.')\ndef test_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_compare_pretokenized_inputs",
        "original": "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    pass",
        "mutated": [
            "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('MarkupLM tokenizer always expects pretokenized inputs.')\ndef test_compare_pretokenized_inputs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_compare_prepare_for_model",
        "original": "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    pass",
        "mutated": [
            "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('MarkupLM fast tokenizer does not support prepare_for_model')\ndef test_compare_prepare_for_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_only_label_first_subword",
        "original": "@slow\ndef test_only_label_first_subword(self):\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])",
        "mutated": [
            "@slow\ndef test_only_label_first_subword(self):\n    if False:\n        i = 10\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])",
            "@slow\ndef test_only_label_first_subword(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])",
            "@slow\ndef test_only_label_first_subword(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])",
            "@slow\ndef test_only_label_first_subword(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])",
            "@slow\ndef test_only_label_first_subword(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nodes = ['hello', 'niels']\n    xpaths = ['/html/body/div/li[1]/div/span' for _ in range(len(nodes))]\n    node_labels = [0, 1]\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, -100, -100])\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base', only_label_first_subword=False)\n    encoding = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels)\n    self.assertListEqual(encoding.labels, [-100, 0, 1, 1, -100])"
        ]
    },
    {
        "func_name": "test_markuplm_integration_test",
        "original": "def test_markuplm_integration_test(self):\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)",
        "mutated": [
            "def test_markuplm_integration_test(self):\n    if False:\n        i = 10\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)",
            "def test_markuplm_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)",
            "def test_markuplm_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)",
            "def test_markuplm_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)",
            "def test_markuplm_integration_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizer_p = MarkupLMTokenizer.from_pretrained('microsoft/markuplm-base')\n    tokenizer_r = MarkupLMTokenizerFast.from_pretrained('microsoft/markuplm-base')\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    node_labels = [1, 2, 3]\n    expected_results = {'input_ids': [0, 42891, 8331, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (nodes, xpaths) = self.get_nodes_and_xpaths_batch()\n    node_labels = [[1, 2, 3], [2, 46, 17, 22, 3]]\n    expected_results = {'input_ids': [[0, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 1, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(nodes, xpaths=xpaths, node_labels=node_labels, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (question, nodes, xpaths) = self.get_question_nodes_and_xpaths()\n    expected_results = {'input_ids': [0, 12196, 18, 39, 766, 116, 2, 42891, 232, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xpath_tags_seq': [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], 'xpath_subs_seq': [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n    encoding_p = tokenizer_p(question, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(question, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)\n    (questions, nodes, xpaths) = self.get_question_nodes_and_xpaths_batch()\n    expected_results = {'input_ids': [[0, 12196, 18, 39, 766, 116, 2, 42891, 232, 12364, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9178, 16, 37, 373, 116, 2, 42891, 127, 766, 16, 22401, 2, 1, 1, 1, 1, 1, 1, 1]], 'xpath_tags_seq': [[[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]], [[216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [109, 25, 50, 120, 50, 178, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216], [216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216, 216]]], 'xpath_subs_seq': [[[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 1, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]], [[1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [0, 0, 0, 2, 0, 0, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001], [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n    encoding_p = tokenizer_p(questions, nodes, xpaths, padding='max_length', max_length=20)\n    encoding_r = tokenizer_r(questions, nodes, xpaths, padding='max_length', max_length=20)\n    self.assertDictEqual(dict(encoding_p), expected_results)\n    self.assertDictEqual(dict(encoding_r), expected_results)"
        ]
    },
    {
        "func_name": "test_np_encode_plus_sent_to_model",
        "original": "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    pass",
        "mutated": [
            "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip(\"Doesn't support another framework than PyTorch\")\ndef test_np_encode_plus_sent_to_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_padding_warning_message_fast_tokenizer",
        "original": "def test_padding_warning_message_fast_tokenizer(self):\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)",
        "mutated": [
            "def test_padding_warning_message_fast_tokenizer(self):\n    if False:\n        i = 10\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)",
            "def test_padding_warning_message_fast_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)",
            "def test_padding_warning_message_fast_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)",
            "def test_padding_warning_message_fast_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)",
            "def test_padding_warning_message_fast_tokenizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.test_rust_tokenizer:\n        return\n    (nodes, xpaths) = self.get_nodes_and_xpaths()\n    tokenizer_fast = self.get_rust_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_fast, nodes)\n    encoding_fast = tokenizer_fast(nodes, xpaths=xpaths)\n    with self.assertLogs('transformers', level='WARNING') as cm:\n        tokenizer_fast.pad(encoding_fast)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.', cm.records[0].message)\n    if not self.test_slow_tokenizer:\n        return\n    tokenizer_slow = self.get_tokenizer()\n    self._check_no_pad_token_padding(tokenizer_slow, nodes)\n    encoding_slow = tokenizer_slow(nodes, xpaths=xpaths)\n    with self.assertLogs(level='WARNING') as cm:\n        logger.warning('Dummy warning')\n        tokenizer_slow.pad(encoding_slow)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Dummy warning', cm.records[0].message)"
        ]
    },
    {
        "func_name": "test_chat_template",
        "original": "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    pass",
        "mutated": [
            "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('Chat is not supported')\ndef test_chat_template(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "test_added_tokens_serialization",
        "original": "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    pass",
        "mutated": [
            "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    if False:\n        i = 10\n    pass",
            "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "@unittest.skip('The model tested fails `Hub -> Fast == Hub -> Slow`, nothing much we can do')\ndef test_added_tokens_serialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    }
]