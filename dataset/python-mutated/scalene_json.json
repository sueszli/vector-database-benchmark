[
    {
        "func_name": "memory_consumed_str",
        "original": "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    \"\"\"Return a string corresponding to amount of memory consumed.\"\"\"\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'",
        "mutated": [
            "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    if False:\n        i = 10\n    'Return a string corresponding to amount of memory consumed.'\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'",
            "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return a string corresponding to amount of memory consumed.'\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'",
            "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return a string corresponding to amount of memory consumed.'\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'",
            "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return a string corresponding to amount of memory consumed.'\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'",
            "@staticmethod\ndef memory_consumed_str(size_in_mb: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return a string corresponding to amount of memory consumed.'\n    gigabytes = size_in_mb // 1024\n    terabytes = gigabytes // 1024\n    if terabytes > 0:\n        return f'{size_in_mb / 1048576:3.3f} TB'\n    elif gigabytes > 0:\n        return f'{size_in_mb / 1024:3.3f} GB'\n    else:\n        return f'{size_in_mb:3.3f} MB'"
        ]
    },
    {
        "func_name": "time_consumed_str",
        "original": "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'",
        "mutated": [
            "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    if False:\n        i = 10\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'",
            "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'",
            "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'",
            "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'",
            "@staticmethod\ndef time_consumed_str(time_in_ms: float) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    hours = time_in_ms // 3600000\n    minutes = time_in_ms % 3600000 // 60000\n    seconds = time_in_ms % 60000 // 1000\n    hours_exact = time_in_ms / 3600000\n    minutes_exact = time_in_ms % 3600000 / 60000\n    seconds_exact = time_in_ms % 60000 / 1000\n    if hours > 0:\n        return f'{hours_exact:.0f}h:{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif minutes > 0:\n        return f'{minutes_exact:.0f}m:{seconds_exact:3.3f}s'\n    elif seconds > 0:\n        return f'{seconds_exact:3.3f}s'\n    else:\n        return f'{time_in_ms:3.3f}ms'"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self) -> None:\n    self.output_file = ''\n    self.gpu = False",
        "mutated": [
            "def __init__(self) -> None:\n    if False:\n        i = 10\n    self.output_file = ''\n    self.gpu = False",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.output_file = ''\n    self.gpu = False",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.output_file = ''\n    self.gpu = False",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.output_file = ''\n    self.gpu = False",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.output_file = ''\n    self.gpu = False"
        ]
    },
    {
        "func_name": "compress_samples",
        "original": "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples",
        "mutated": [
            "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if False:\n        i = 10\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples",
            "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples",
            "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples",
            "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples",
            "def compress_samples(self, samples: List[Any], max_footprint: float) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(samples) <= self.max_sparkline_samples:\n        return samples\n    epsilon = len(samples) / (3 * self.max_sparkline_samples) * 2\n    new_samples = rdp(samples, epsilon=epsilon)\n    if len(new_samples) > self.max_sparkline_samples:\n        new_samples = sorted(random.sample(new_samples, self.max_sparkline_samples))\n    return new_samples"
        ]
    },
    {
        "func_name": "output_profile_line",
        "original": "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    \"\"\"Print at most one line of the profile (true == printed one).\"\"\"\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}",
        "mutated": [
            "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n    'Print at most one line of the profile (true == printed one).'\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}",
            "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Print at most one line of the profile (true == printed one).'\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}",
            "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Print at most one line of the profile (true == printed one).'\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}",
            "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Print at most one line of the profile (true == printed one).'\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}",
            "def output_profile_line(self, *, fname: Filename, fname_print: Filename, line_no: LineNumber, line: str, stats: ScaleneStatistics, profile_this_code: Callable[[Filename, LineNumber], bool], profile_memory: bool=False, force_print: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Print at most one line of the profile (true == printed one).'\n    if not force_print and (not profile_this_code(fname, line_no)):\n        return {'lineno': line_no, 'line': line, 'n_core_utilization': 0, 'n_cpu_percent_c': 0, 'n_cpu_percent_python': 0, 'n_sys_percent': 0, 'n_gpu_percent': 0, 'n_gpu_avg_memory_mb': 0, 'n_gpu_peak_memory_mb': 0, 'n_peak_mb': 0, 'n_growth_mb': 0, 'n_avg_mb': 0, 'n_mallocs': 0, 'n_malloc_mb': 0, 'n_usage_fraction': 0, 'n_python_fraction': 0, 'n_copy_mb_s': 0, 'memory_samples': []}\n    n_cpu_samples_c = stats.cpu_samples_c[fname][line_no]\n    n_cpu_samples_c = max(0, n_cpu_samples_c)\n    n_cpu_samples_python = stats.cpu_samples_python[fname][line_no]\n    n_gpu_samples = stats.gpu_samples[fname][line_no]\n    n_gpu_mem_samples = stats.gpu_mem_samples[fname][line_no]\n    if stats.total_cpu_samples:\n        n_cpu_percent_c = n_cpu_samples_c * 100 / stats.total_cpu_samples\n        n_cpu_percent_python = n_cpu_samples_python * 100 / stats.total_cpu_samples\n    else:\n        n_cpu_percent_c = 0\n        n_cpu_percent_python = 0\n    if stats.total_gpu_samples:\n        n_gpu_percent = n_gpu_samples * 100 / stats.total_gpu_samples\n    else:\n        n_gpu_percent = 0\n    n_malloc_mb = stats.memory_malloc_samples[fname][line_no]\n    n_mallocs = stats.memory_malloc_count[fname][line_no]\n    n_python_malloc_mb = stats.memory_python_samples[fname][line_no]\n    n_usage_fraction = 0 if not stats.total_memory_malloc_samples else n_malloc_mb / stats.total_memory_malloc_samples\n    n_python_fraction = 0 if not n_malloc_mb else n_python_malloc_mb / n_malloc_mb\n    n_avg_mb = stats.memory_aggregate_footprint[fname][line_no] if n_mallocs == 0 else stats.memory_aggregate_footprint[fname][line_no] / n_mallocs\n    n_peak_mb = stats.memory_max_footprint[fname][line_no]\n    if n_avg_mb > n_peak_mb:\n        n_avg_mb = n_peak_mb\n    n_cpu_percent = n_cpu_percent_c + n_cpu_percent_python\n    mean_cpu_util = stats.cpu_utilization[fname][line_no].mean()\n    mean_core_util = stats.core_utilization[fname][line_no].mean()\n    n_sys_percent = n_cpu_percent * (1.0 - mean_cpu_util)\n    n_cpu_percent_python *= mean_cpu_util\n    n_cpu_percent_c *= mean_cpu_util\n    del mean_cpu_util\n    n_copy_b = stats.memcpy_samples[fname][line_no]\n    if stats.elapsed_time:\n        n_copy_mb_s = n_copy_b / (1024 * 1024 * stats.elapsed_time)\n    else:\n        n_copy_mb_s = 0\n    stats.per_line_footprint_samples[fname][line_no] = self.compress_samples(stats.per_line_footprint_samples[fname][line_no], stats.max_footprint)\n    return {'lineno': line_no, 'line': line, 'n_core_utilization': mean_core_util, 'n_cpu_percent_c': n_cpu_percent_c, 'n_cpu_percent_python': n_cpu_percent_python, 'n_sys_percent': n_sys_percent, 'n_gpu_percent': n_gpu_percent, 'n_gpu_avg_memory_mb': n_gpu_mem_samples.mean(), 'n_gpu_peak_memory_mb': n_gpu_mem_samples.peak(), 'n_peak_mb': n_peak_mb, 'n_growth_mb': n_peak_mb, 'n_avg_mb': n_avg_mb, 'n_mallocs': n_mallocs, 'n_malloc_mb': n_malloc_mb, 'n_usage_fraction': n_usage_fraction, 'n_python_fraction': n_python_fraction, 'n_copy_mb_s': n_copy_mb_s, 'memory_samples': stats.per_line_footprint_samples[fname][line_no]}"
        ]
    },
    {
        "func_name": "output_profiles",
        "original": "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    \"\"\"Write the profile out.\"\"\"\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output",
        "mutated": [
            "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n    'Write the profile out.'\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output",
            "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Write the profile out.'\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output",
            "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Write the profile out.'\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output",
            "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Write the profile out.'\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output",
            "def output_profiles(self, program: Filename, stats: ScaleneStatistics, pid: int, profile_this_code: Callable[[Filename, LineNumber], bool], python_alias_dir: Path, program_path: Path, entrypoint_dir: Path, program_args: Optional[List[str]], profile_memory: bool=True, reduced_profile: bool=False) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Write the profile out.'\n    if not pid:\n        stats.merge_stats(python_alias_dir)\n    if not stats.total_cpu_samples and (not stats.total_memory_malloc_samples) and (not stats.total_memory_free_samples):\n        return {}\n    all_instrumented_files: List[Filename] = list(set(list(stats.cpu_samples_python.keys()) + list(stats.cpu_samples_c.keys()) + list(stats.memory_free_samples.keys()) + list(stats.memory_malloc_samples.keys())))\n    if not all_instrumented_files:\n        return {}\n    growth_rate = 0.0\n    if profile_memory:\n        stats.memory_footprint_samples = self.compress_samples(stats.memory_footprint_samples, stats.max_footprint)\n        if stats.allocation_velocity[1] > 0:\n            growth_rate = 100.0 * stats.allocation_velocity[0] / stats.allocation_velocity[1]\n    else:\n        stats.memory_footprint_samples = []\n    result = re.match('_ipython-input-([0-9]+)-.*', program)\n    if result:\n        program = Filename('[' + result.group(1) + ']')\n    stks = []\n    for stk in stats.stacks.keys():\n        this_stk: List[str] = []\n        this_stk.extend(stk)\n        stks.append((this_stk, stats.stacks[stk]))\n    output: Dict[str, Any] = {'program': program, 'entrypoint_dir': entrypoint_dir, 'args': program_args, 'filename': program_path, 'alloc_samples': stats.alloc_samples, 'elapsed_time_sec': stats.elapsed_time, 'growth_rate': growth_rate, 'max_footprint_mb': stats.max_footprint, 'max_footprint_fname': stats.max_footprint_loc[0] if stats.max_footprint_loc else None, 'max_footprint_lineno': stats.max_footprint_loc[1] if stats.max_footprint_loc else None, 'files': {}, 'gpu': self.gpu, 'memory': profile_memory, 'samples': stats.memory_footprint_samples, 'stacks': stks}\n    report_files: List[Filename] = []\n    for fname in sorted(all_instrumented_files, key=lambda f: (-stats.cpu_samples[f], f)):\n        fname = Filename(fname)\n        try:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.elapsed_time\n        except ZeroDivisionError:\n            percent_cpu_time = 0\n        if stats.malloc_samples[fname] < self.malloc_threshold and percent_cpu_time < self.cpu_percent_threshold:\n            continue\n        report_files.append(fname)\n    if pid:\n        stats.output_stats(pid, python_alias_dir)\n        return {'is_child': True}\n    if len(report_files) == 0:\n        return {}\n    for fname in report_files:\n        fname_print = fname\n        result = re.match('_ipython-input-([0-9]+)-.*', fname_print)\n        if result:\n            fname_print = Filename('[' + result.group(1) + ']')\n        avg_mallocs: Dict[LineNumber, float] = defaultdict(float)\n        for line_no in stats.bytei_map[fname]:\n            n_malloc_mb = stats.memory_aggregate_footprint[fname][line_no]\n            count = stats.memory_malloc_count[fname][line_no]\n            if count:\n                avg_mallocs[line_no] = n_malloc_mb / count\n            else:\n                avg_mallocs[line_no] = n_malloc_mb\n        avg_mallocs = OrderedDict(sorted(avg_mallocs.items(), key=itemgetter(1), reverse=True))\n        leaks = ScaleneLeakAnalysis.compute_leaks(growth_rate, stats, avg_mallocs, fname)\n        leaks = sorted(leaks, key=itemgetter(1), reverse=True)\n        reported_leaks = {}\n        for (leak_lineno, leak_likelihood, leak_velocity) in leaks:\n            reported_leaks[str(leak_lineno)] = {'likelihood': leak_likelihood, 'velocity_mb_s': leak_velocity / stats.elapsed_time}\n        if not stats.total_cpu_samples:\n            percent_cpu_time = 0\n        else:\n            percent_cpu_time = 100 * stats.cpu_samples[fname] / stats.total_cpu_samples\n        full_fname = fname\n        try:\n            with open(full_fname, 'r', encoding='utf-8') as source_file:\n                code_lines = source_file.readlines()\n        except (FileNotFoundError, OSError):\n            continue\n        code_str = ''.join(code_lines)\n        enclosing_regions = ScaleneAnalysis.find_regions(code_str)\n        outer_loop = ScaleneAnalysis.find_outermost_loop(code_str)\n        imports = ScaleneAnalysis.get_native_imported_modules(code_str)\n        output['files'][fname_print] = {'percent_cpu_time': percent_cpu_time, 'lines': [], 'leaks': reported_leaks, 'imports': imports}\n        for (lineno, line) in enumerate(code_lines, start=1):\n            line = line.replace('&', '\\\\u0026')\n            line = line.replace('<', '\\\\u003c')\n            line = line.replace('>', '\\\\u003e')\n            profile_line = self.output_profile_line(fname=fname, fname_print=fname_print, line_no=LineNumber(lineno), line=line, stats=stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=False)\n            if profile_line:\n                profile_line['start_region_line'] = enclosing_regions[lineno][0]\n                profile_line['end_region_line'] = enclosing_regions[lineno][1]\n                profile_line['start_outermost_loop'] = outer_loop[lineno][0]\n                profile_line['end_outermost_loop'] = outer_loop[lineno][1]\n                if reduced_profile:\n                    profile_line_copy = copy.copy(profile_line)\n                    del profile_line_copy['line']\n                    del profile_line_copy['lineno']\n                    if not any(profile_line_copy.values()):\n                        continue\n                output['files'][fname_print]['lines'].append(profile_line)\n        fn_stats = stats.build_function_stats(fname)\n        print_fn_summary = False\n        all_samples = set()\n        all_samples |= set(fn_stats.cpu_samples_python.keys())\n        all_samples |= set(fn_stats.cpu_samples_c.keys())\n        all_samples |= set(fn_stats.memory_malloc_samples.keys())\n        all_samples |= set(fn_stats.memory_free_samples.keys())\n        print_fn_summary = any((fn != fname for fn in all_samples))\n        output['files'][fname_print]['functions'] = []\n        if print_fn_summary:\n            for fn_name in sorted(all_samples, key=lambda k: stats.firstline_map[k]):\n                if fn_name == fname:\n                    continue\n                profile_line = self.output_profile_line(fname=fn_name, fname_print=fn_name, line_no=LineNumber(1), line=fn_name, stats=fn_stats, profile_this_code=profile_this_code, profile_memory=profile_memory, force_print=True)\n                if profile_line:\n                    profile_line['lineno'] = stats.firstline_map[fn_name]\n                    output['files'][fname_print]['functions'].append(profile_line)\n    return output"
        ]
    }
]