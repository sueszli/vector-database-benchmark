[
    {
        "func_name": "__init__",
        "original": "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    \"\"\"Ctor.\"\"\"\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
        "mutated": [
            "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n    'Ctor.'\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Ctor.'\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Ctor.'\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Ctor.'\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, p=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01, 0.1), exp_thresh=-1.0, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Ctor.'\n    del kwargs\n    if p < 0.0 or p > 1.0:\n        raise ValueError('p must be in [0, 1]')\n    self.num_players = None\n    self.p_init = p\n    self.p = p\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = True\n    self.aux_errors = []\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)"
        ]
    },
    {
        "func_name": "init_vars",
        "original": "def init_vars(self, num_strats, num_players):\n    \"\"\"Initialize solver parameters.\"\"\"\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)",
        "mutated": [
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if len(num_strats) != num_players:\n        raise ValueError('Must specify num strategies for each player')\n    init_dist = []\n    for num_strats_i in num_strats:\n        if self.rnd_init:\n            init_dist_i = self.random.rand(num_strats_i)\n        else:\n            init_dist_i = np.ones(num_strats_i)\n        init_dist_i /= init_dist_i.sum()\n        init_dist.append(init_dist_i)\n    init_y = [np.zeros_like(dist_i) for dist_i in init_dist]\n    init_anneal_steps = 0\n    return (init_dist, init_y, init_anneal_steps)"
        ]
    },
    {
        "func_name": "record_aux_errors",
        "original": "def record_aux_errors(self, grads):\n    \"\"\"Record errors for the auxiliary variables.\"\"\"\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])",
        "mutated": [
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n    'Record errors for the auxiliary variables.'\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Record errors for the auxiliary variables.'\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Record errors for the auxiliary variables.'\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Record errors for the auxiliary variables.'\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])",
            "def record_aux_errors(self, grads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Record errors for the auxiliary variables.'\n    grad_y = grads[1]\n    grad_y_flat = np.concatenate([np.ravel(g) for g in grad_y])\n    self.aux_errors.append([np.linalg.norm(grad_y_flat)])"
        ]
    },
    {
        "func_name": "compute_gradients",
        "original": "def compute_gradients(self, params, payoff_matrices):\n    \"\"\"Compute and return gradients (and exploitabilities) for all parameters.\n\n    Args:\n      params: tuple of params (dist, y, anneal_steps), see gradients\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n    Returns:\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)",
        "mutated": [
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, y, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, y, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, y, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, y, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, y, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_y, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap:\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.p, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.p, self.proj_grad)"
        ]
    },
    {
        "func_name": "exploitability",
        "original": "def exploitability(self, params, payoff_matrices):\n    \"\"\"Compute and return tsallis entropy regularized exploitability.\n\n    Args:\n      params: tuple of params (dist, y), see ate.gradients\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n    Returns:\n      float, exploitability of current dist\n    \"\"\"\n    return exp.ate_exploitability(params, payoff_matrices, self.p)",
        "mutated": [
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Compute and return tsallis entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see ate.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.ate_exploitability(params, payoff_matrices, self.p)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute and return tsallis entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see ate.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.ate_exploitability(params, payoff_matrices, self.p)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute and return tsallis entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see ate.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.ate_exploitability(params, payoff_matrices, self.p)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute and return tsallis entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see ate.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.ate_exploitability(params, payoff_matrices, self.p)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute and return tsallis entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see ate.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.ate_exploitability(params, payoff_matrices, self.p)"
        ]
    },
    {
        "func_name": "euc_descent_step",
        "original": "def euc_descent_step(self, params, grads, t):\n    \"\"\"Projected gradient descent on exploitability using Euclidean projection.\n\n    Args:\n      params: tuple of variables to be updated (dist, y, anneal_steps)\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\n      t: int, solver iteration (unused)\n    Returns:\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\n    \"\"\"\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
        "mutated": [
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_y, grad_anneal_steps)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y, new_anneal_steps)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = dist_i - lr_dist * dist_grad_i\n        new_dist_i = simplex.euclidean_projection_onto_simplex(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)"
        ]
    },
    {
        "func_name": "mirror_descent_step",
        "original": "def mirror_descent_step(self, params, grads, t):\n    \"\"\"Entropic mirror descent on exploitability.\n\n    Args:\n      params: tuple of variables to be updated (dist, y)\n      grads: tuple of variable gradients (grad_dist, grad_y)\n      t: int, solver iteration (unused)\n    Returns:\n      new_params: tuple of update params (new_dist, new_y)\n    \"\"\"\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
        "mutated": [
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y)\\n      grads: tuple of variable gradients (grad_dist, grad_y)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y)\\n      grads: tuple of variable gradients (grad_dist, grad_y)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y)\\n      grads: tuple of variable gradients (grad_dist, grad_y)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y)\\n      grads: tuple of variable gradients (grad_dist, grad_y)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, y)\\n      grads: tuple of variable gradients (grad_dist, grad_y)\\n      t: int, solver iteration (unused)\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_y)\\n    '\n    (lr_dist, lr_y) = self.lrs\n    new_dist = []\n    for (dist_i, dist_grad_i) in zip(params[0], grads[0]):\n        new_dist_i = np.log(np.clip(dist_i, 0.0, np.inf)) - lr_dist * dist_grad_i\n        new_dist_i = special.softmax(new_dist_i)\n        new_dist.append(new_dist_i)\n    lr_y = np.clip(1 / float(t + 1), lr_y, np.inf)\n    new_y = []\n    for (y_i, y_grad_i) in zip(params[1], grads[1]):\n        new_y_i = y_i - lr_y * y_grad_i\n        new_y_i = np.clip(new_y_i, 0.0, np.inf)\n        new_y.append(new_y_i)\n    new_anneal_steps = params[2] + grads[2]\n    return (new_dist, new_y, new_anneal_steps)"
        ]
    },
    {
        "func_name": "gradients",
        "original": "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    \"\"\"Computes exploitablity gradient and aux variable gradients.\n\n    Args:\n      dist: list of 1-d np.arrays, current estimate of nash distribution\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\n      anneal_steps: int, elapsed num steps since last anneal\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n          are sorted and arrays should be indexed in the same order\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\n      proj_grad: bool, if True, projects dist gradient onto simplex\n    Returns:\n      gradient of exploitability w.r.t. (dist, y) as tuple\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
        "mutated": [
            "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n    \"Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def gradients(self, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        nabla_i = np.zeros_like(dist[i])\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_i_ij = payoff_matrices[i, j][0]\n            else:\n                hess_i_ij = payoff_matrices[j, i][1].T\n            nabla_ij = hess_i_ij.dot(dist[j])\n            nabla_i += nabla_ij / float(num_players - 1)\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            grad_dist_i += hess_j_ij.dot(other_player_fx[j])\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)"
        ]
    },
    {
        "func_name": "cheap_gradients",
        "original": "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    \"\"\"Computes exploitablity gradient and aux variable gradients with samples.\n\n    This implementation takes payoff_matrices as input so technically uses\n    O(d^2) compute but only a single column of payoff_matrices is used to\n    perform the update so can be re-implemented in O(d) if needed.\n\n    Args:\n      random: random number generator, np.random.RandomState(seed)\n      dist: list of 1-d np.arrays, current estimate of nash distribution\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\n      anneal_steps: int, elapsed num steps since last anneal\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\n          are sorted and arrays should be indexed in the same order\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\n      proj_grad: bool, if True, projects dist gradient onto simplex\n    Returns:\n      gradient of exploitability w.r.t. (dist, y) as tuple\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
        "mutated": [
            "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n    \"Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)",
            "def cheap_gradients(self, random, dist, y, anneal_steps, payoff_matrices, num_players, p=1, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: list of 1-d np.arrays, current estimate of nash distribution\\n      y: list 1-d np.arrays (same shape as dist), current est. of payoff grad\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: dictionary with keys as tuples of agents (i, j) and\\n          values of (2 x A x A) np.arrays, payoffs for each joint action. keys\\n          are sorted and arrays should be indexed in the same order\\n      num_players: int, number of players, in case payoff_matrices is abbrev'd\\n      p: float in [0, 1], Tsallis entropy-regularization --> 0 as p --> 0\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, y) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    \"\n    policy_gradient = []\n    other_player_fx = []\n    grad_y = []\n    unreg_exp = []\n    reg_exp = []\n    for i in range(num_players):\n        others = list(range(num_players))\n        others.remove(i)\n        j = np.random.choice(others)\n        action_j = random.choice(dist[j].size, p=dist[j])\n        if i < j:\n            hess_i_ij = payoff_matrices[i, j][0]\n        else:\n            hess_i_ij = payoff_matrices[j, i][1].T\n        nabla_i = hess_i_ij[:, action_j]\n        grad_y.append(y[i] - nabla_i)\n        if p > 0.01:\n            power = 1.0 / float(p)\n            s_i = np.linalg.norm(y[i], ord=power)\n            if s_i == 0:\n                br_i = misc.uniform_dist(y[i])\n            else:\n                br_i = (y[i] / s_i) ** power\n        else:\n            power = np.inf\n            s_i = np.linalg.norm(y[i], ord=power)\n            br_i = np.zeros_like(dist[i])\n            maxima_i = y[i] == s_i\n            br_i[maxima_i] = 1.0 / maxima_i.sum()\n        policy_gradient_i = nabla_i - s_i * dist[i] ** p\n        policy_gradient.append(policy_gradient_i)\n        unreg_exp.append(np.max(y[i]) - y[i].dot(dist[i]))\n        br_i_inv_sparse = 1 - np.sum(br_i ** (p + 1))\n        dist_i_inv_sparse = 1 - np.sum(dist[i] ** (p + 1))\n        entr_br_i = s_i / (p + 1) * br_i_inv_sparse\n        entr_dist_i = s_i / (p + 1) * dist_i_inv_sparse\n        reg_exp.append(y[i].dot(br_i - dist[i]) + entr_br_i - entr_dist_i)\n        entr_br_vec_i = br_i_inv_sparse * br_i ** (1 - p)\n        entr_dist_vec_i = dist_i_inv_sparse * dist[i] ** (1 - p)\n        other_player_fx_i = br_i - dist[i] + 1 / (p + 1) * (entr_br_vec_i - entr_dist_vec_i)\n        other_player_fx.append(other_player_fx_i)\n    grad_dist = []\n    for i in range(num_players):\n        grad_dist_i = -policy_gradient[i]\n        for j in range(num_players):\n            if j == i:\n                continue\n            if i < j:\n                hess_j_ij = payoff_matrices[i, j][1]\n            else:\n                hess_j_ij = payoff_matrices[j, i][0].T\n            action_u = random.choice(dist[j].size)\n            other_player_fx_j = dist[j].size * other_player_fx[j][action_u]\n            grad_dist_i += hess_j_ij[:, action_u] * other_player_fx_j\n        if proj_grad:\n            grad_dist_i = simplex.project_grad(grad_dist_i)\n        grad_dist.append(grad_dist_i)\n    unreg_exp_mean = np.mean(unreg_exp)\n    reg_exp_mean = np.mean(reg_exp)\n    (_, lr_y) = self.lrs\n    if reg_exp_mean < self.exp_thresh and anneal_steps >= 1 / lr_y:\n        self.p = np.clip(p / 2.0, 0.0, 1.0)\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_y, grad_anneal_steps), unreg_exp_mean, reg_exp_mean)"
        ]
    }
]