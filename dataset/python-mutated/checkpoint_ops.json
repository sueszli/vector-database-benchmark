[
    {
        "func_name": "_load_and_remap_matrix",
        "original": "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    \"\"\"Loads a 2-D (matrix) `Tensor` from checkpoint.\n\n  Generates 1D-remappings for rows and columns using the\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\n  matrix that loads existing values from the checkpoint, while filling out\n  \"missing\" values with the newly initialized values. See\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\n  row remapping or only col remapping. If only row remapping is desired,\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\n  remapping.\n\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\n  (row axis) via `new_row_vocab_offset`.\n\n  Args:\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\n      from which the old matrix `Tensor` will be loaded.\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\n    new_row_vocab_offset: A 0-indexed integer representing what line to\n      start reading at in the new row vocabulary. Used for partitioned\n      variables.\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\n      support variable partitioning and partial loading, this does not need to\n      be the same as the number of entries in `new_row_vocab_file`).\n    new_col_vocab_size: Number of columns to load - should be the same as the\n      number of entries in `new_col_vocab_file`, since we don't support\n      partitioning along the column axis.\n    initializer: Callable initializer function that accepts a 1-D tensor as the\n      arg to specify the shape of the returned tensor. Used to initialize\n      missing values.\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\n      With the default value of -1, the entire old row vocabulary file will be\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\n      considered for remapping.Must be smaller than the length of\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\n      `old_col_vocab_size` for classes.\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the old row vocabulary file. Can be None, which represents no\n      remapping on the row axis.\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\n      to the new row vocabulary file. Can be None, which represents no remapping\n      on the row axis - in which case, `new_row_vocab_offset` and\n      `num_rows_to_load` work under the assumption that the new row vocab is the\n      same as the old row vocab.\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the old column vocabulary file. Can be None, which represents no\n      remapping on the column axis.\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\n      to the new column vocabulary file. Can be None, which represents no\n      remapping on the column axis - in which case, `new_col_vocab_size` works\n      under the assumption that the new col vocab is the same as the old col\n      vocab.\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\n      to append. Must be >= 0.\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\n      columns to append. Must be >= 0.\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\n      be loaded into memory. Setting this arg trades increased disk reads for\n      lower memory usage.\n\n  Returns:\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\n    specified tensor in the checkpoint, and any missing or OOV values\n    initialized with the given `initializer`.\n\n  Raises:\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\n      provided, while the other is not. Same for `old_col_vocab_file` and\n      `new_col_vocab_file`.\n    ValueError: If neither row vocabs or col vocabs are provided.\n  \"\"\"\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor",
        "mutated": [
            "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    if False:\n        i = 10\n    'Loads a 2-D (matrix) `Tensor` from checkpoint.\\n\\n  Generates 1D-remappings for rows and columns using the\\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\\n  matrix that loads existing values from the checkpoint, while filling out\\n  \"missing\" values with the newly initialized values. See\\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\\n  row remapping or only col remapping. If only row remapping is desired,\\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\\n  remapping.\\n\\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\\n  (row axis) via `new_row_vocab_offset`.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_offset: A 0-indexed integer representing what line to\\n      start reading at in the new row vocabulary. Used for partitioned\\n      variables.\\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\\n      support variable partitioning and partial loading, this does not need to\\n      be the same as the number of entries in `new_row_vocab_file`).\\n    new_col_vocab_size: Number of columns to load - should be the same as the\\n      number of entries in `new_col_vocab_file`, since we don\\'t support\\n      partitioning along the column axis.\\n    initializer: Callable initializer function that accepts a 1-D tensor as the\\n      arg to specify the shape of the returned tensor. Used to initialize\\n      missing values.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis - in which case, `new_row_vocab_offset` and\\n      `num_rows_to_load` work under the assumption that the new row vocab is the\\n      same as the old row vocab.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis - in which case, `new_col_vocab_size` works\\n      under the assumption that the new col vocab is the same as the old col\\n      vocab.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\\n    specified tensor in the checkpoint, and any missing or OOV values\\n    initialized with the given `initializer`.\\n\\n  Raises:\\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\\n      provided, while the other is not. Same for `old_col_vocab_file` and\\n      `new_col_vocab_file`.\\n    ValueError: If neither row vocabs or col vocabs are provided.\\n  '\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor",
            "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Loads a 2-D (matrix) `Tensor` from checkpoint.\\n\\n  Generates 1D-remappings for rows and columns using the\\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\\n  matrix that loads existing values from the checkpoint, while filling out\\n  \"missing\" values with the newly initialized values. See\\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\\n  row remapping or only col remapping. If only row remapping is desired,\\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\\n  remapping.\\n\\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\\n  (row axis) via `new_row_vocab_offset`.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_offset: A 0-indexed integer representing what line to\\n      start reading at in the new row vocabulary. Used for partitioned\\n      variables.\\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\\n      support variable partitioning and partial loading, this does not need to\\n      be the same as the number of entries in `new_row_vocab_file`).\\n    new_col_vocab_size: Number of columns to load - should be the same as the\\n      number of entries in `new_col_vocab_file`, since we don\\'t support\\n      partitioning along the column axis.\\n    initializer: Callable initializer function that accepts a 1-D tensor as the\\n      arg to specify the shape of the returned tensor. Used to initialize\\n      missing values.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis - in which case, `new_row_vocab_offset` and\\n      `num_rows_to_load` work under the assumption that the new row vocab is the\\n      same as the old row vocab.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis - in which case, `new_col_vocab_size` works\\n      under the assumption that the new col vocab is the same as the old col\\n      vocab.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\\n    specified tensor in the checkpoint, and any missing or OOV values\\n    initialized with the given `initializer`.\\n\\n  Raises:\\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\\n      provided, while the other is not. Same for `old_col_vocab_file` and\\n      `new_col_vocab_file`.\\n    ValueError: If neither row vocabs or col vocabs are provided.\\n  '\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor",
            "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Loads a 2-D (matrix) `Tensor` from checkpoint.\\n\\n  Generates 1D-remappings for rows and columns using the\\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\\n  matrix that loads existing values from the checkpoint, while filling out\\n  \"missing\" values with the newly initialized values. See\\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\\n  row remapping or only col remapping. If only row remapping is desired,\\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\\n  remapping.\\n\\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\\n  (row axis) via `new_row_vocab_offset`.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_offset: A 0-indexed integer representing what line to\\n      start reading at in the new row vocabulary. Used for partitioned\\n      variables.\\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\\n      support variable partitioning and partial loading, this does not need to\\n      be the same as the number of entries in `new_row_vocab_file`).\\n    new_col_vocab_size: Number of columns to load - should be the same as the\\n      number of entries in `new_col_vocab_file`, since we don\\'t support\\n      partitioning along the column axis.\\n    initializer: Callable initializer function that accepts a 1-D tensor as the\\n      arg to specify the shape of the returned tensor. Used to initialize\\n      missing values.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis - in which case, `new_row_vocab_offset` and\\n      `num_rows_to_load` work under the assumption that the new row vocab is the\\n      same as the old row vocab.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis - in which case, `new_col_vocab_size` works\\n      under the assumption that the new col vocab is the same as the old col\\n      vocab.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\\n    specified tensor in the checkpoint, and any missing or OOV values\\n    initialized with the given `initializer`.\\n\\n  Raises:\\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\\n      provided, while the other is not. Same for `old_col_vocab_file` and\\n      `new_col_vocab_file`.\\n    ValueError: If neither row vocabs or col vocabs are provided.\\n  '\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor",
            "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Loads a 2-D (matrix) `Tensor` from checkpoint.\\n\\n  Generates 1D-remappings for rows and columns using the\\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\\n  matrix that loads existing values from the checkpoint, while filling out\\n  \"missing\" values with the newly initialized values. See\\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\\n  row remapping or only col remapping. If only row remapping is desired,\\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\\n  remapping.\\n\\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\\n  (row axis) via `new_row_vocab_offset`.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_offset: A 0-indexed integer representing what line to\\n      start reading at in the new row vocabulary. Used for partitioned\\n      variables.\\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\\n      support variable partitioning and partial loading, this does not need to\\n      be the same as the number of entries in `new_row_vocab_file`).\\n    new_col_vocab_size: Number of columns to load - should be the same as the\\n      number of entries in `new_col_vocab_file`, since we don\\'t support\\n      partitioning along the column axis.\\n    initializer: Callable initializer function that accepts a 1-D tensor as the\\n      arg to specify the shape of the returned tensor. Used to initialize\\n      missing values.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis - in which case, `new_row_vocab_offset` and\\n      `num_rows_to_load` work under the assumption that the new row vocab is the\\n      same as the old row vocab.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis - in which case, `new_col_vocab_size` works\\n      under the assumption that the new col vocab is the same as the old col\\n      vocab.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\\n    specified tensor in the checkpoint, and any missing or OOV values\\n    initialized with the given `initializer`.\\n\\n  Raises:\\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\\n      provided, while the other is not. Same for `old_col_vocab_file` and\\n      `new_col_vocab_file`.\\n    ValueError: If neither row vocabs or col vocabs are provided.\\n  '\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor",
            "def _load_and_remap_matrix(ckpt_path, old_tensor_name, new_row_vocab_offset, num_rows_to_load, new_col_vocab_size, initializer, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Loads a 2-D (matrix) `Tensor` from checkpoint.\\n\\n  Generates 1D-remappings for rows and columns using the\\n  `GenerateVocabRemapping` op, and initializes any anticipated values with the\\n  provided initializer. Then, uses the `LoadAndRemapMatrix` op to create a\\n  matrix that loads existing values from the checkpoint, while filling out\\n  \"missing\" values with the newly initialized values. See\\n  contrib/framework/ops/checkpoint_ops.cc for more information on the wrapped\\n  functionality (LoadAndRemapMatrix). This wrapper can be used to perform only\\n  row remapping or only col remapping. If only row remapping is desired,\\n  {new,old}_col_vocab_file should be `None`, and vice versa for column\\n  remapping.\\n\\n  NOTE: This only supports div-partitioning the vocabulary on the 1st dimension\\n  (row axis) via `new_row_vocab_offset`.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_offset: A 0-indexed integer representing what line to\\n      start reading at in the new row vocabulary. Used for partitioned\\n      variables.\\n    num_rows_to_load: Number of rows to load for the new vocabulary (note: to\\n      support variable partitioning and partial loading, this does not need to\\n      be the same as the number of entries in `new_row_vocab_file`).\\n    new_col_vocab_size: Number of columns to load - should be the same as the\\n      number of entries in `new_col_vocab_file`, since we don\\'t support\\n      partitioning along the column axis.\\n    initializer: Callable initializer function that accepts a 1-D tensor as the\\n      arg to specify the shape of the returned tensor. Used to initialize\\n      missing values.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis - in which case, `new_row_vocab_offset` and\\n      `num_rows_to_load` work under the assumption that the new row vocab is the\\n      same as the old row vocab.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis - in which case, `new_col_vocab_size` works\\n      under the assumption that the new col vocab is the same as the old col\\n      vocab.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A Tensor of shape `[num_rows_to_load + num_row_oov_buckets,\\n    new_col_vocab_size + num_col_oov_buckets]`, with values loaded from the\\n    specified tensor in the checkpoint, and any missing or OOV values\\n    initialized with the given `initializer`.\\n\\n  Raises:\\n    ValueError: If `num_row_oov_buckets` or `num_col_oov_buckets` < 0.\\n    ValueError: If either `old_row_vocab_file` or `new_row_vocab_file` is\\n      provided, while the other is not. Same for `old_col_vocab_file` and\\n      `new_col_vocab_file`.\\n    ValueError: If neither row vocabs or col vocabs are provided.\\n  '\n    if num_row_oov_buckets < 0:\n        raise ValueError('num_row_oov_buckets must be >= 0, but received %d' % num_row_oov_buckets)\n    if num_col_oov_buckets < 0:\n        raise ValueError('num_col_oov_buckets must be >= 0, but received %d' % num_col_oov_buckets)\n    if bool(old_row_vocab_file) != bool(new_row_vocab_file):\n        raise ValueError(\"old_row_vocab_file and new_row_vocab_file must both be specified or left unspecified. old_row_vocab_file='{}', new_row_vocab_file='{}'\".format(old_row_vocab_file, new_row_vocab_file))\n    if bool(old_col_vocab_file) != bool(new_col_vocab_file):\n        raise ValueError(\"old_col_vocab_file and new_col_vocab_file must both be specified or left unspecified. old_col_vocab_file='{}', new_col_vocab_file='{}'\".format(old_col_vocab_file, new_col_vocab_file))\n    remap_rows = new_row_vocab_file and old_row_vocab_file\n    remap_cols = new_col_vocab_file and old_col_vocab_file\n    if not (remap_rows or remap_cols):\n        raise ValueError('Must provide either row or column vocab files. If no remapping is necessary, consider using `tf.contrib.framework.init_from_checkpoint` instead.')\n    num_rows_present = num_rows_to_load\n    if remap_rows:\n        (row_remapping, num_rows_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_row_vocab_file, old_vocab_file=old_row_vocab_file, new_vocab_offset=new_row_vocab_offset, num_new_vocab=num_rows_to_load, old_vocab_size=old_row_vocab_size)\n    else:\n        row_remapping = math_ops.range(new_row_vocab_offset, new_row_vocab_offset + num_rows_to_load, dtype=dtypes.int64)\n    col_remapping = []\n    num_cols_present = new_col_vocab_size\n    if remap_cols:\n        (col_remapping, num_cols_present) = gen_checkpoint_ops.generate_vocab_remapping(new_vocab_file=new_col_vocab_file, old_vocab_file=old_col_vocab_file, new_vocab_offset=0, num_new_vocab=new_col_vocab_size)\n    init_vals = initializer([num_rows_to_load * new_col_vocab_size - num_rows_present * num_cols_present, 1])\n    return_tensor = gen_checkpoint_ops.load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=init_vals, num_rows=num_rows_to_load, num_cols=new_col_vocab_size, max_rows_in_memory=max_rows_in_memory)\n    if num_row_oov_buckets > 0:\n        init_row_oov_val = initializer([num_row_oov_buckets, new_col_vocab_size])\n        init_row_oov_val = ops.convert_to_tensor(init_row_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_row_oov_val], 0)\n    if num_col_oov_buckets > 0:\n        init_col_oov_val = initializer([num_rows_to_load + num_row_oov_buckets, num_col_oov_buckets])\n        init_col_oov_val = ops.convert_to_tensor(init_col_oov_val)\n        return_tensor = array_ops.concat([return_tensor, init_col_oov_val], 1)\n    return return_tensor"
        ]
    },
    {
        "func_name": "_initializer",
        "original": "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)",
        "mutated": [
            "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    if False:\n        i = 10\n    'Variable initializer.\\n\\n    Args:\\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\\n      dtype: Must be float32.\\n      partition_info: variable_scope._PartitionInfo.\\n\\n    Returns:\\n      `Tensor` of shape `shape`.\\n\\n    Raises:\\n      TypeError: If `dtype` is anything other than float32.\\n      ValueError: For shape mismatch upon invocation.\\n    '\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)",
            "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Variable initializer.\\n\\n    Args:\\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\\n      dtype: Must be float32.\\n      partition_info: variable_scope._PartitionInfo.\\n\\n    Returns:\\n      `Tensor` of shape `shape`.\\n\\n    Raises:\\n      TypeError: If `dtype` is anything other than float32.\\n      ValueError: For shape mismatch upon invocation.\\n    '\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)",
            "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Variable initializer.\\n\\n    Args:\\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\\n      dtype: Must be float32.\\n      partition_info: variable_scope._PartitionInfo.\\n\\n    Returns:\\n      `Tensor` of shape `shape`.\\n\\n    Raises:\\n      TypeError: If `dtype` is anything other than float32.\\n      ValueError: For shape mismatch upon invocation.\\n    '\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)",
            "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Variable initializer.\\n\\n    Args:\\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\\n      dtype: Must be float32.\\n      partition_info: variable_scope._PartitionInfo.\\n\\n    Returns:\\n      `Tensor` of shape `shape`.\\n\\n    Raises:\\n      TypeError: If `dtype` is anything other than float32.\\n      ValueError: For shape mismatch upon invocation.\\n    '\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)",
            "def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Variable initializer.\\n\\n    Args:\\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\\n      dtype: Must be float32.\\n      partition_info: variable_scope._PartitionInfo.\\n\\n    Returns:\\n      `Tensor` of shape `shape`.\\n\\n    Raises:\\n      TypeError: If `dtype` is anything other than float32.\\n      ValueError: For shape mismatch upon invocation.\\n    '\n    if dtype != dtypes.float32:\n        raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n    if len(shape) != 2:\n        raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n    if shape[0] <= 0:\n        raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n    if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n        raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n    offset = 0\n    if partition_info is not None:\n        offset = partition_info.single_offset(shape)\n    if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n        raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n    row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n    num_rows_to_load = shape[0] - row_oov_buckets_to_use\n    if offset > new_row_vocab_size:\n        if shape[0] != row_oov_buckets_to_use:\n            raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n        return initializer(shape)\n    return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)"
        ]
    },
    {
        "func_name": "_load_and_remap_matrix_initializer",
        "original": "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    \"\"\"Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\n\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\n  rows/columns according to the specified vocab files and append additional\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\n\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\n  a text file, with each line containing a single entity within the vocabulary.\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\n  line i of file f. Then, row i of the new output matrix will be taken from row\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\n  matrix. If any entity in `new_row_vocab_file` is not found in\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\n  will be initialized using the `initializer` arg. The same logic also applies\n  for the columns.\n\n  For example, assuming that:\n\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\n  * `w(i, j)` represents the value from row i, column j of the old matrix\n\n  Then the new output matrix will look like:\n\n  `[[w(1, 0), w(1, 2), 1],\n    [2,       3,       4],\n    [w(0, 0), w(0, 2), 5]]`\n\n  If we further specify that:\n\n  * `num_row_oov_buckets` == 2\n  * `num_col_oov_buckets` == 1\n\n  Then the new output matrix will look like:\n\n  `[[w(1, 0), w(1, 2), 1,  12],\n    [2,       3,       4,  13],\n    [w(0, 0), w(0, 2), 5,  14],\n    [6,       7,       8,  15],\n    [9,       10,      11, 16]]`\n\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\n  vocab files are the same, and no row remapping is done. If\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\n  vocab files are the same, and no column remapping is done.\n\n  The returned initializer only supports div-partitioning along the row axis. It\n  does not support partitioning along the column axis (as this is not common in\n  practice) or mod-partitioning.\n\n  NOTE: When this is used to warm-start variables, client code should use\n  `tf.lookup.index_table_from_tensor()` like\n  contrib/layers/python/layers/feature_column.py does, as opposed to\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\n  same.\n\n  Args:\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\n      from which the old matrix `Tensor` will be loaded.\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\n    new_row_vocab_size: `int` specifying the number of entries in\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\n      provided), this should be equal to the number of rows to load from the old\n      matrix (which can theoretically be smaller than the number of rows in the\n      old matrix).\n    new_col_vocab_size: `int` specifying the number of entries in\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\n      provided), this should be equal to the number of columns in the old\n      matrix.\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\n      With the default value of -1, the entire old row vocabulary file will be\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\n      considered for remapping.Must be smaller than the length of\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\n      `old_col_vocab_size` for classes.\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the old row vocabulary file. Can be None, which represents no\n      remapping on the row axis.\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\n      to the new row vocabulary file. Can be None, which represents no remapping\n      on the row axis.\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the old column vocabulary file. Can be None, which represents no\n      remapping on the column axis.\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\n      to the new column vocabulary file. Can be None, which represents no\n      remapping on the column axis.\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\n      to append. Must be >= 0.\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\n      columns to append. Must be >= 0.\n    initializer: Initializer function to initialize missing values. Accepts a\n      1-D tensor as the arg to specify the shape of the returned tensor. If\n      `None`, defaults to using `zeros_initializer()`.\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\n      be loaded into memory. Setting this arg trades increased disk reads for\n      lower memory usage.\n\n  Returns:\n    A variable initializer function that should be used to initialize a\n    (potentially partitioned) `Variable` whose complete shape is\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\n    num_col_oov_buckets]`.\n\n  Raises:\n    TypeError: If `initializer` is specified but not callable.\n  \"\"\"\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer",
        "mutated": [
            "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n    'Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\\n\\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\\n  rows/columns according to the specified vocab files and append additional\\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\\n\\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\\n  a text file, with each line containing a single entity within the vocabulary.\\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\\n  line i of file f. Then, row i of the new output matrix will be taken from row\\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\\n  matrix. If any entity in `new_row_vocab_file` is not found in\\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\\n  will be initialized using the `initializer` arg. The same logic also applies\\n  for the columns.\\n\\n  For example, assuming that:\\n\\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\\n  * `w(i, j)` represents the value from row i, column j of the old matrix\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1],\\n    [2,       3,       4],\\n    [w(0, 0), w(0, 2), 5]]`\\n\\n  If we further specify that:\\n\\n  * `num_row_oov_buckets` == 2\\n  * `num_col_oov_buckets` == 1\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1,  12],\\n    [2,       3,       4,  13],\\n    [w(0, 0), w(0, 2), 5,  14],\\n    [6,       7,       8,  15],\\n    [9,       10,      11, 16]]`\\n\\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\\n  vocab files are the same, and no row remapping is done. If\\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\\n  vocab files are the same, and no column remapping is done.\\n\\n  The returned initializer only supports div-partitioning along the row axis. It\\n  does not support partitioning along the column axis (as this is not common in\\n  practice) or mod-partitioning.\\n\\n  NOTE: When this is used to warm-start variables, client code should use\\n  `tf.lookup.index_table_from_tensor()` like\\n  contrib/layers/python/layers/feature_column.py does, as opposed to\\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\\n  same.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_size: `int` specifying the number of entries in\\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\\n      provided), this should be equal to the number of rows to load from the old\\n      matrix (which can theoretically be smaller than the number of rows in the\\n      old matrix).\\n    new_col_vocab_size: `int` specifying the number of entries in\\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\\n      provided), this should be equal to the number of columns in the old\\n      matrix.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    initializer: Initializer function to initialize missing values. Accepts a\\n      1-D tensor as the arg to specify the shape of the returned tensor. If\\n      `None`, defaults to using `zeros_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function that should be used to initialize a\\n    (potentially partitioned) `Variable` whose complete shape is\\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\\n    num_col_oov_buckets]`.\\n\\n  Raises:\\n    TypeError: If `initializer` is specified but not callable.\\n  '\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer",
            "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\\n\\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\\n  rows/columns according to the specified vocab files and append additional\\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\\n\\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\\n  a text file, with each line containing a single entity within the vocabulary.\\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\\n  line i of file f. Then, row i of the new output matrix will be taken from row\\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\\n  matrix. If any entity in `new_row_vocab_file` is not found in\\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\\n  will be initialized using the `initializer` arg. The same logic also applies\\n  for the columns.\\n\\n  For example, assuming that:\\n\\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\\n  * `w(i, j)` represents the value from row i, column j of the old matrix\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1],\\n    [2,       3,       4],\\n    [w(0, 0), w(0, 2), 5]]`\\n\\n  If we further specify that:\\n\\n  * `num_row_oov_buckets` == 2\\n  * `num_col_oov_buckets` == 1\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1,  12],\\n    [2,       3,       4,  13],\\n    [w(0, 0), w(0, 2), 5,  14],\\n    [6,       7,       8,  15],\\n    [9,       10,      11, 16]]`\\n\\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\\n  vocab files are the same, and no row remapping is done. If\\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\\n  vocab files are the same, and no column remapping is done.\\n\\n  The returned initializer only supports div-partitioning along the row axis. It\\n  does not support partitioning along the column axis (as this is not common in\\n  practice) or mod-partitioning.\\n\\n  NOTE: When this is used to warm-start variables, client code should use\\n  `tf.lookup.index_table_from_tensor()` like\\n  contrib/layers/python/layers/feature_column.py does, as opposed to\\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\\n  same.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_size: `int` specifying the number of entries in\\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\\n      provided), this should be equal to the number of rows to load from the old\\n      matrix (which can theoretically be smaller than the number of rows in the\\n      old matrix).\\n    new_col_vocab_size: `int` specifying the number of entries in\\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\\n      provided), this should be equal to the number of columns in the old\\n      matrix.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    initializer: Initializer function to initialize missing values. Accepts a\\n      1-D tensor as the arg to specify the shape of the returned tensor. If\\n      `None`, defaults to using `zeros_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function that should be used to initialize a\\n    (potentially partitioned) `Variable` whose complete shape is\\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\\n    num_col_oov_buckets]`.\\n\\n  Raises:\\n    TypeError: If `initializer` is specified but not callable.\\n  '\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer",
            "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\\n\\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\\n  rows/columns according to the specified vocab files and append additional\\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\\n\\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\\n  a text file, with each line containing a single entity within the vocabulary.\\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\\n  line i of file f. Then, row i of the new output matrix will be taken from row\\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\\n  matrix. If any entity in `new_row_vocab_file` is not found in\\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\\n  will be initialized using the `initializer` arg. The same logic also applies\\n  for the columns.\\n\\n  For example, assuming that:\\n\\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\\n  * `w(i, j)` represents the value from row i, column j of the old matrix\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1],\\n    [2,       3,       4],\\n    [w(0, 0), w(0, 2), 5]]`\\n\\n  If we further specify that:\\n\\n  * `num_row_oov_buckets` == 2\\n  * `num_col_oov_buckets` == 1\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1,  12],\\n    [2,       3,       4,  13],\\n    [w(0, 0), w(0, 2), 5,  14],\\n    [6,       7,       8,  15],\\n    [9,       10,      11, 16]]`\\n\\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\\n  vocab files are the same, and no row remapping is done. If\\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\\n  vocab files are the same, and no column remapping is done.\\n\\n  The returned initializer only supports div-partitioning along the row axis. It\\n  does not support partitioning along the column axis (as this is not common in\\n  practice) or mod-partitioning.\\n\\n  NOTE: When this is used to warm-start variables, client code should use\\n  `tf.lookup.index_table_from_tensor()` like\\n  contrib/layers/python/layers/feature_column.py does, as opposed to\\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\\n  same.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_size: `int` specifying the number of entries in\\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\\n      provided), this should be equal to the number of rows to load from the old\\n      matrix (which can theoretically be smaller than the number of rows in the\\n      old matrix).\\n    new_col_vocab_size: `int` specifying the number of entries in\\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\\n      provided), this should be equal to the number of columns in the old\\n      matrix.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    initializer: Initializer function to initialize missing values. Accepts a\\n      1-D tensor as the arg to specify the shape of the returned tensor. If\\n      `None`, defaults to using `zeros_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function that should be used to initialize a\\n    (potentially partitioned) `Variable` whose complete shape is\\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\\n    num_col_oov_buckets]`.\\n\\n  Raises:\\n    TypeError: If `initializer` is specified but not callable.\\n  '\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer",
            "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\\n\\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\\n  rows/columns according to the specified vocab files and append additional\\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\\n\\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\\n  a text file, with each line containing a single entity within the vocabulary.\\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\\n  line i of file f. Then, row i of the new output matrix will be taken from row\\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\\n  matrix. If any entity in `new_row_vocab_file` is not found in\\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\\n  will be initialized using the `initializer` arg. The same logic also applies\\n  for the columns.\\n\\n  For example, assuming that:\\n\\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\\n  * `w(i, j)` represents the value from row i, column j of the old matrix\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1],\\n    [2,       3,       4],\\n    [w(0, 0), w(0, 2), 5]]`\\n\\n  If we further specify that:\\n\\n  * `num_row_oov_buckets` == 2\\n  * `num_col_oov_buckets` == 1\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1,  12],\\n    [2,       3,       4,  13],\\n    [w(0, 0), w(0, 2), 5,  14],\\n    [6,       7,       8,  15],\\n    [9,       10,      11, 16]]`\\n\\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\\n  vocab files are the same, and no row remapping is done. If\\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\\n  vocab files are the same, and no column remapping is done.\\n\\n  The returned initializer only supports div-partitioning along the row axis. It\\n  does not support partitioning along the column axis (as this is not common in\\n  practice) or mod-partitioning.\\n\\n  NOTE: When this is used to warm-start variables, client code should use\\n  `tf.lookup.index_table_from_tensor()` like\\n  contrib/layers/python/layers/feature_column.py does, as opposed to\\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\\n  same.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_size: `int` specifying the number of entries in\\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\\n      provided), this should be equal to the number of rows to load from the old\\n      matrix (which can theoretically be smaller than the number of rows in the\\n      old matrix).\\n    new_col_vocab_size: `int` specifying the number of entries in\\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\\n      provided), this should be equal to the number of columns in the old\\n      matrix.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    initializer: Initializer function to initialize missing values. Accepts a\\n      1-D tensor as the arg to specify the shape of the returned tensor. If\\n      `None`, defaults to using `zeros_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function that should be used to initialize a\\n    (potentially partitioned) `Variable` whose complete shape is\\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\\n    num_col_oov_buckets]`.\\n\\n  Raises:\\n    TypeError: If `initializer` is specified but not callable.\\n  '\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer",
            "def _load_and_remap_matrix_initializer(ckpt_path, old_tensor_name, new_row_vocab_size, new_col_vocab_size, old_row_vocab_size=-1, old_row_vocab_file=None, new_row_vocab_file=None, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=0, num_col_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a var initializer for loading and remapping a 2-D (matrix) tensor.\\n\\n  The returned initializer loads a 2-D (matrix) `Tensor` with name\\n  `old_tensor_name` from the checkpoint at `ckpt_path`. It will reorder the\\n  rows/columns according to the specified vocab files and append additional\\n  out-of-vocabulary rows/columns according to the number of OOV buckets.\\n\\n  The format of the file at the `{old,new}_{row,col}_vocab_file` path should be\\n  a text file, with each line containing a single entity within the vocabulary.\\n  Let the function `line_of(f, \"x\")` return the 0-indexed line number of the\\n  entity \"x\" in file f, and the function `entity_at(f, i)` return the entity at\\n  line i of file f. Then, row i of the new output matrix will be taken from row\\n  `line_of(old_row_vocab_file, entity_at(new_row_vocab_file, i))` of the old\\n  matrix. If any entity in `new_row_vocab_file` is not found in\\n  `old_row_vocab_file`, that row is considered a \"missing\" row, and its values\\n  will be initialized using the `initializer` arg. The same logic also applies\\n  for the columns.\\n\\n  For example, assuming that:\\n\\n  * `old_row_vocab_file` contains \"mercury\\\\nvenus\\\\nmars\"\\n  * `new_row_vocab_file` contains \"venus\\\\njupiter\\\\nmercury\"\\n  * `old_col_vocab_file` contains \"good\\\\nbetter\\\\nbest\"\\n  * `new_col_vocab_file` contains \"good\\\\nbest\\\\nfantastic\"\\n  * `initializer` returns the natural numbers `[1, 2, 3, 4, ...]`\\n  * `w(i, j)` represents the value from row i, column j of the old matrix\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1],\\n    [2,       3,       4],\\n    [w(0, 0), w(0, 2), 5]]`\\n\\n  If we further specify that:\\n\\n  * `num_row_oov_buckets` == 2\\n  * `num_col_oov_buckets` == 1\\n\\n  Then the new output matrix will look like:\\n\\n  `[[w(1, 0), w(1, 2), 1,  12],\\n    [2,       3,       4,  13],\\n    [w(0, 0), w(0, 2), 5,  14],\\n    [6,       7,       8,  15],\\n    [9,       10,      11, 16]]`\\n\\n  If `{old,new}_row_vocab_file` are None, we assume that the old and new row\\n  vocab files are the same, and no row remapping is done. If\\n  `{old,new}_col_vocab_file` are None, we assume that the old and new column\\n  vocab files are the same, and no column remapping is done.\\n\\n  The returned initializer only supports div-partitioning along the row axis. It\\n  does not support partitioning along the column axis (as this is not common in\\n  practice) or mod-partitioning.\\n\\n  NOTE: When this is used to warm-start variables, client code should use\\n  `tf.lookup.index_table_from_tensor()` like\\n  contrib/layers/python/layers/feature_column.py does, as opposed to\\n  `tf.feature_to_id()` - in order to ensure the underlying lookup tables are the\\n  same.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    old_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_row_vocab_size: `int` specifying the number of entries in\\n      `new_row_vocab_file`. If no row remapping is needed (no row vocab\\n      provided), this should be equal to the number of rows to load from the old\\n      matrix (which can theoretically be smaller than the number of rows in the\\n      old matrix).\\n    new_col_vocab_size: `int` specifying the number of entries in\\n      `new_col_vocab_file`. If no column remapping is needed (no column vocab\\n      provided), this should be equal to the number of columns in the old\\n      matrix.\\n    old_row_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_row_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.  NOTE: we do not provide an equivalent\\n      `old_col_vocab_size` for classes.\\n    old_row_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old row vocabulary file. Can be None, which represents no\\n      remapping on the row axis.\\n    new_row_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new row vocabulary file. Can be None, which represents no remapping\\n      on the row axis.\\n    old_col_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    new_col_vocab_file: A scalar `Tensor` of type `string` containing the path\\n      to the new column vocabulary file. Can be None, which represents no\\n      remapping on the column axis.\\n    num_row_oov_buckets: `int` specifying the number of out-of-vocabulary rows\\n      to append. Must be >= 0.\\n    num_col_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      columns to append. Must be >= 0.\\n    initializer: Initializer function to initialize missing values. Accepts a\\n      1-D tensor as the arg to specify the shape of the returned tensor. If\\n      `None`, defaults to using `zeros_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function that should be used to initialize a\\n    (potentially partitioned) `Variable` whose complete shape is\\n    `[new_row_vocab_size + num_row_oov_buckets, new_col_vocab_size +\\n    num_col_oov_buckets]`.\\n\\n  Raises:\\n    TypeError: If `initializer` is specified but not callable.\\n  '\n    if initializer is None:\n        initializer = init_ops.zeros_initializer()\n    if not callable(initializer):\n        raise TypeError('initializer must be callable, instead of being {} of type {}.'.format(initializer, type(initializer)))\n\n    def _initializer(shape, dtype=dtypes.float32, partition_info=None):\n        \"\"\"Variable initializer.\n\n    Args:\n      shape: Shape of `Tensor` to return. Should include OOV on both axes.\n      dtype: Must be float32.\n      partition_info: variable_scope._PartitionInfo.\n\n    Returns:\n      `Tensor` of shape `shape`.\n\n    Raises:\n      TypeError: If `dtype` is anything other than float32.\n      ValueError: For shape mismatch upon invocation.\n    \"\"\"\n        if dtype != dtypes.float32:\n            raise TypeError('Currently, only float32 is supported. Received dtype: {}'.format(dtype))\n        if len(shape) != 2:\n            raise ValueError('Expected 2-dim shape, but received: {}'.format(shape))\n        if shape[0] <= 0:\n            raise ValueError('Expected 1st dim of shape to be > 0, but received shape: {}'.format(shape))\n        if shape[1] != new_col_vocab_size + num_col_oov_buckets:\n            raise ValueError('Expected 2nd dim of shape to be new_col_vocab_size ({}) + num_col_oov_buckets ({}) = {}, but received shape: {}'.format(new_col_vocab_size, num_col_oov_buckets, new_col_vocab_size + num_col_oov_buckets, shape))\n        offset = 0\n        if partition_info is not None:\n            offset = partition_info.single_offset(shape)\n        if offset + shape[0] > new_row_vocab_size + num_row_oov_buckets:\n            raise ValueError('Trying to initialize {} additional rows after {} rows have already been initialized, which would exceed expected total row count of new_row_vocab_size ({}) + num_row_oov_buckets ({}) = {}.'.format(shape[0], offset, new_row_vocab_size, num_row_oov_buckets, new_row_vocab_size + num_row_oov_buckets))\n        row_oov_buckets_to_use = min(shape[0], max(0, offset + shape[0] - new_row_vocab_size))\n        num_rows_to_load = shape[0] - row_oov_buckets_to_use\n        if offset > new_row_vocab_size:\n            if shape[0] != row_oov_buckets_to_use:\n                raise ValueError('Partitioned variable offset is greater than new vocab size and not operating on OOV-only partition.')\n            return initializer(shape)\n        return _load_and_remap_matrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, new_row_vocab_offset=offset, num_rows_to_load=num_rows_to_load, new_col_vocab_size=new_col_vocab_size, initializer=initializer, old_row_vocab_size=old_row_vocab_size, old_row_vocab_file=old_row_vocab_file, new_row_vocab_file=new_row_vocab_file, old_col_vocab_file=old_col_vocab_file, new_col_vocab_file=new_col_vocab_file, num_row_oov_buckets=row_oov_buckets_to_use, num_col_oov_buckets=num_col_oov_buckets, max_rows_in_memory=max_rows_in_memory)\n    return _initializer"
        ]
    },
    {
        "func_name": "_load_embedding_initializer",
        "original": "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    \"\"\"Returns a variable initializer for loading pre-trained embeddings.\n\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\n  embedding weights and remapping according to the provided vocab files. See\n  docs for `load_and_remap_matrix_initializer()` for more details.\n\n  NOTE: Only for use with div-partitioned variables / vocabularies.\n\n  Args:\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\n      from which the old matrix `Tensor` will be loaded.\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\n    new_vocab_size: Number of entries in the new vocab.\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\n      the checkpoint. Must match the number of columns in the old embedding\n      matrix.\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the old vocabulary file.\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\n      path to the new vocabulary file.\n    old_vocab_size: The number of entries to consider in the old vocabulary.\n      With the default value of -1, the entire old row vocabulary file will be\n      used.  Otherwise, only the first `old_vocab_size` entries will be\n      considered for remapping.Must be smaller than the length of\n      `old_row_vocab_file`.\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\n      buckets to use. Must be >= 0.\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\n      specify the shape of the returned tensor. If `None`, defaults to using\n      `truncated_normal_initializer()`.\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\n      be loaded into memory. Setting this arg trades increased disk reads for\n      lower memory usage.\n\n  Returns:\n    A variable initializer function.\n  \"\"\"\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)",
        "mutated": [
            "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n    'Returns a variable initializer for loading pre-trained embeddings.\\n\\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\\n  embedding weights and remapping according to the provided vocab files. See\\n  docs for `load_and_remap_matrix_initializer()` for more details.\\n\\n  NOTE: Only for use with div-partitioned variables / vocabularies.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_vocab_size: Number of entries in the new vocab.\\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\\n      the checkpoint. Must match the number of columns in the old embedding\\n      matrix.\\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old vocabulary file.\\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the new vocabulary file.\\n    old_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.\\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      buckets to use. Must be >= 0.\\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\\n      specify the shape of the returned tensor. If `None`, defaults to using\\n      `truncated_normal_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function.\\n  '\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)",
            "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a variable initializer for loading pre-trained embeddings.\\n\\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\\n  embedding weights and remapping according to the provided vocab files. See\\n  docs for `load_and_remap_matrix_initializer()` for more details.\\n\\n  NOTE: Only for use with div-partitioned variables / vocabularies.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_vocab_size: Number of entries in the new vocab.\\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\\n      the checkpoint. Must match the number of columns in the old embedding\\n      matrix.\\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old vocabulary file.\\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the new vocabulary file.\\n    old_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.\\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      buckets to use. Must be >= 0.\\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\\n      specify the shape of the returned tensor. If `None`, defaults to using\\n      `truncated_normal_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function.\\n  '\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)",
            "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a variable initializer for loading pre-trained embeddings.\\n\\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\\n  embedding weights and remapping according to the provided vocab files. See\\n  docs for `load_and_remap_matrix_initializer()` for more details.\\n\\n  NOTE: Only for use with div-partitioned variables / vocabularies.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_vocab_size: Number of entries in the new vocab.\\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\\n      the checkpoint. Must match the number of columns in the old embedding\\n      matrix.\\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old vocabulary file.\\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the new vocabulary file.\\n    old_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.\\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      buckets to use. Must be >= 0.\\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\\n      specify the shape of the returned tensor. If `None`, defaults to using\\n      `truncated_normal_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function.\\n  '\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)",
            "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a variable initializer for loading pre-trained embeddings.\\n\\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\\n  embedding weights and remapping according to the provided vocab files. See\\n  docs for `load_and_remap_matrix_initializer()` for more details.\\n\\n  NOTE: Only for use with div-partitioned variables / vocabularies.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_vocab_size: Number of entries in the new vocab.\\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\\n      the checkpoint. Must match the number of columns in the old embedding\\n      matrix.\\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old vocabulary file.\\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the new vocabulary file.\\n    old_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.\\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      buckets to use. Must be >= 0.\\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\\n      specify the shape of the returned tensor. If `None`, defaults to using\\n      `truncated_normal_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function.\\n  '\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)",
            "def _load_embedding_initializer(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size=-1, num_oov_buckets=0, initializer=None, max_rows_in_memory=-1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a variable initializer for loading pre-trained embeddings.\\n\\n  Wrapper around `load_and_remap_matrix_initializer()` specialized for loading\\n  embedding weights and remapping according to the provided vocab files. See\\n  docs for `load_and_remap_matrix_initializer()` for more details.\\n\\n  NOTE: Only for use with div-partitioned variables / vocabularies.\\n\\n  Args:\\n    ckpt_path: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\\n      from which the old matrix `Tensor` will be loaded.\\n    embedding_tensor_name: Name of the 2-D `Tensor` to load from checkpoint.\\n    new_vocab_size: Number of entries in the new vocab.\\n    embedding_dim: `int` specifying the dimension of the embedding vectors from\\n      the checkpoint. Must match the number of columns in the old embedding\\n      matrix.\\n    old_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the old vocabulary file.\\n    new_vocab_file: A scalar `Tensor` of type `string` containing the\\n      path to the new vocabulary file.\\n    old_vocab_size: The number of entries to consider in the old vocabulary.\\n      With the default value of -1, the entire old row vocabulary file will be\\n      used.  Otherwise, only the first `old_vocab_size` entries will be\\n      considered for remapping.Must be smaller than the length of\\n      `old_row_vocab_file`.\\n    num_oov_buckets: `int` specifying the number of out-of-vocabulary\\n      buckets to use. Must be >= 0.\\n    initializer: Initializer function that accepts a 1-D tensor as the arg to\\n      specify the shape of the returned tensor. If `None`, defaults to using\\n      `truncated_normal_initializer()`.\\n    max_rows_in_memory: `int` specifying the maximum number of rows to load from\\n      the checkpoint at once. If less than or equal to 0, the entire matrix will\\n      be loaded into memory. Setting this arg trades increased disk reads for\\n      lower memory usage.\\n\\n  Returns:\\n    A variable initializer function.\\n  '\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(stddev=1.0 / math.sqrt(embedding_dim))\n    return _load_and_remap_matrix_initializer(ckpt_path=ckpt_path, old_tensor_name=embedding_tensor_name, new_row_vocab_size=new_vocab_size, new_col_vocab_size=embedding_dim, old_row_vocab_size=old_vocab_size, old_row_vocab_file=old_vocab_file, new_row_vocab_file=new_vocab_file, old_col_vocab_file=None, new_col_vocab_file=None, num_row_oov_buckets=num_oov_buckets, num_col_oov_buckets=0, initializer=initializer, max_rows_in_memory=max_rows_in_memory)"
        ]
    }
]