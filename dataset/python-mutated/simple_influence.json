[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale",
        "mutated": [
            "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    if False:\n        i = 10\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale",
            "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale",
            "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale",
            "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale",
            "def __init__(self, model: Model, train_data_path: DatasetReaderInput, train_dataset_reader: DatasetReader, *, test_dataset_reader: Optional[DatasetReader]=None, train_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), test_data_loader: Lazy[DataLoader]=Lazy(SimpleDataLoader.from_dataset_reader), params_to_freeze: List[str]=None, cuda_device: int=-1, lissa_batch_size: int=8, damping: float=0.003, num_samples: int=1, recursion_depth: Union[float, int]=0.25, scale: float=10000.0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(model=model, train_data_path=train_data_path, train_dataset_reader=train_dataset_reader, test_dataset_reader=test_dataset_reader, train_data_loader=train_data_loader, test_data_loader=test_data_loader, params_to_freeze=params_to_freeze, cuda_device=cuda_device)\n    self._lissa_dataloader = SimpleDataLoader(list(self._train_loader.iter_instances()), lissa_batch_size, shuffle=True, vocab=self.vocab)\n    self._lissa_dataloader.set_target_device(self.device)\n    if isinstance(recursion_depth, float) and recursion_depth > 0.0:\n        self._lissa_dataloader.batches_per_epoch = int(len(self._lissa_dataloader) * recursion_depth)\n    elif isinstance(recursion_depth, int) and recursion_depth > 0:\n        self._lissa_dataloader.batches_per_epoch = recursion_depth\n    else:\n        raise ValueError(\"'recursion_depth' should be a positive int or float\")\n    self._damping = damping\n    self._num_samples = num_samples\n    self._recursion_depth = recursion_depth\n    self._scale = scale"
        ]
    },
    {
        "func_name": "_calculate_influence_scores",
        "original": "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]",
        "mutated": [
            "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    if False:\n        i = 10\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]",
            "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]",
            "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]",
            "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]",
            "def _calculate_influence_scores(self, test_instance: Instance, test_loss: float, test_grads: Sequence[torch.Tensor]) -> List[float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    inv_hvp = get_inverse_hvp_lissa(test_grads, self.model, self.used_params, self._lissa_dataloader, self._damping, self._num_samples, self._scale)\n    return [torch.dot(inv_hvp, _flatten_tensors(x.grads)).item() for x in Tqdm.tqdm(self.train_instances, desc='scoring train instances')]"
        ]
    },
    {
        "func_name": "get_inverse_hvp_lissa",
        "original": "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    \"\"\"\n    This function approximates the product of the inverse of the Hessian and\n    the vectors `vs` using LiSSA.\n\n    Adapted from [github.com/kohpangwei/influence-release]\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\n    and [github.com/xhan77/influence-function-analysis]\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\n    \"\"\"\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp",
        "mutated": [
            "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    This function approximates the product of the inverse of the Hessian and\\n    the vectors `vs` using LiSSA.\\n\\n    Adapted from [github.com/kohpangwei/influence-release]\\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\\n    and [github.com/xhan77/influence-function-analysis]\\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\\n    '\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp",
            "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function approximates the product of the inverse of the Hessian and\\n    the vectors `vs` using LiSSA.\\n\\n    Adapted from [github.com/kohpangwei/influence-release]\\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\\n    and [github.com/xhan77/influence-function-analysis]\\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\\n    '\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp",
            "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function approximates the product of the inverse of the Hessian and\\n    the vectors `vs` using LiSSA.\\n\\n    Adapted from [github.com/kohpangwei/influence-release]\\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\\n    and [github.com/xhan77/influence-function-analysis]\\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\\n    '\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp",
            "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function approximates the product of the inverse of the Hessian and\\n    the vectors `vs` using LiSSA.\\n\\n    Adapted from [github.com/kohpangwei/influence-release]\\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\\n    and [github.com/xhan77/influence-function-analysis]\\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\\n    '\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp",
            "def get_inverse_hvp_lissa(vs: Sequence[torch.Tensor], model: Model, used_params: Sequence[torch.Tensor], lissa_data_loader: DataLoader, damping: float, num_samples: int, scale: float) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function approximates the product of the inverse of the Hessian and\\n    the vectors `vs` using LiSSA.\\n\\n    Adapted from [github.com/kohpangwei/influence-release]\\n    (https://github.com/kohpangwei/influence-release/blob/0f656964867da6ddcca16c14b3e4f0eef38a7472/influence/genericNeuralNet.py#L475),\\n    the repo for [Koh, P.W., & Liang, P. (2017)](https://api.semanticscholar.org/CorpusID:13193974),\\n    and [github.com/xhan77/influence-function-analysis]\\n    (https://github.com/xhan77/influence-function-analysis/blob/78d5a967aba885f690d34e88d68da8678aee41f1/bert_util.py#L336),\\n    the repo for [Han, Xiaochuang et al. (2020)](https://api.semanticscholar.org/CorpusID:218628619).\\n    '\n    inverse_hvps = [torch.tensor(0) for _ in vs]\n    for _ in Tqdm.tqdm(range(num_samples), desc='LiSSA samples', total=num_samples):\n        cur_estimates = vs\n        recursion_iter = Tqdm.tqdm(lissa_data_loader, desc='LiSSA depth', total=len(lissa_data_loader))\n        for (j, training_batch) in enumerate(recursion_iter):\n            model.zero_grad()\n            train_output_dict = model(**training_batch)\n            hvps = get_hvp(train_output_dict['loss'], used_params, cur_estimates)\n            cur_estimates = [v + (1 - damping) * cur_estimate - hvp / scale for (v, cur_estimate, hvp) in zip(vs, cur_estimates, hvps)]\n            if j % 50 == 0 or j == len(lissa_data_loader) - 1:\n                norm = np.linalg.norm(_flatten_tensors(cur_estimates).cpu().numpy())\n                recursion_iter.set_description(desc=f'calculating inverse HVP, norm = {norm:.5f}')\n        inverse_hvps = [inverse_hvp + cur_estimate / scale for (inverse_hvp, cur_estimate) in zip(inverse_hvps, cur_estimates)]\n    return_ihvp = _flatten_tensors(inverse_hvps)\n    return_ihvp /= num_samples\n    return return_ihvp"
        ]
    },
    {
        "func_name": "get_hvp",
        "original": "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    \"\"\"\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\n    with respect to the one of the parameter tensors in `params` and the corresponding\n    vector `v` in `vectors`.\n\n    # Parameters\n\n    loss : `torch.Tensor`\n        The loss calculated from the output of the model.\n    params : `Sequence[torch.Tensor]`\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\n        with respect to.\n    vectors : `Sequence[torch.Tensor]`\n        The list of vectors for calculating the HVP.\n    \"\"\"\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp",
        "mutated": [
            "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    if False:\n        i = 10\n    '\\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\\n    with respect to the one of the parameter tensors in `params` and the corresponding\\n    vector `v` in `vectors`.\\n\\n    # Parameters\\n\\n    loss : `torch.Tensor`\\n        The loss calculated from the output of the model.\\n    params : `Sequence[torch.Tensor]`\\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\\n        with respect to.\\n    vectors : `Sequence[torch.Tensor]`\\n        The list of vectors for calculating the HVP.\\n    '\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp",
            "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\\n    with respect to the one of the parameter tensors in `params` and the corresponding\\n    vector `v` in `vectors`.\\n\\n    # Parameters\\n\\n    loss : `torch.Tensor`\\n        The loss calculated from the output of the model.\\n    params : `Sequence[torch.Tensor]`\\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\\n        with respect to.\\n    vectors : `Sequence[torch.Tensor]`\\n        The list of vectors for calculating the HVP.\\n    '\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp",
            "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\\n    with respect to the one of the parameter tensors in `params` and the corresponding\\n    vector `v` in `vectors`.\\n\\n    # Parameters\\n\\n    loss : `torch.Tensor`\\n        The loss calculated from the output of the model.\\n    params : `Sequence[torch.Tensor]`\\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\\n        with respect to.\\n    vectors : `Sequence[torch.Tensor]`\\n        The list of vectors for calculating the HVP.\\n    '\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp",
            "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\\n    with respect to the one of the parameter tensors in `params` and the corresponding\\n    vector `v` in `vectors`.\\n\\n    # Parameters\\n\\n    loss : `torch.Tensor`\\n        The loss calculated from the output of the model.\\n    params : `Sequence[torch.Tensor]`\\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\\n        with respect to.\\n    vectors : `Sequence[torch.Tensor]`\\n        The list of vectors for calculating the HVP.\\n    '\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp",
            "def get_hvp(loss: torch.Tensor, params: Sequence[torch.Tensor], vectors: Sequence[torch.Tensor]) -> Tuple[torch.Tensor, ...]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get a Hessian-Vector Product (HVP) `Hv` for each Hessian `H` of the `loss`\\n    with respect to the one of the parameter tensors in `params` and the corresponding\\n    vector `v` in `vectors`.\\n\\n    # Parameters\\n\\n    loss : `torch.Tensor`\\n        The loss calculated from the output of the model.\\n    params : `Sequence[torch.Tensor]`\\n        Tunable and used parameters in the model that we will calculate the gradient and hessian\\n        with respect to.\\n    vectors : `Sequence[torch.Tensor]`\\n        The list of vectors for calculating the HVP.\\n    '\n    assert len(params) == len(vectors)\n    assert all((p.size() == v.size() for (p, v) in zip(params, vectors)))\n    grads = autograd.grad(loss, params, create_graph=True, retain_graph=True)\n    hvp = autograd.grad(grads, params, grad_outputs=vectors)\n    return hvp"
        ]
    },
    {
        "func_name": "_flatten_tensors",
        "original": "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Unwraps a list of parameters gradients\n\n    # Returns\n\n    `torch.Tensor`\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\n    \"\"\"\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)",
        "mutated": [
            "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Unwraps a list of parameters gradients\\n\\n    # Returns\\n\\n    `torch.Tensor`\\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\\n    '\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)",
            "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Unwraps a list of parameters gradients\\n\\n    # Returns\\n\\n    `torch.Tensor`\\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\\n    '\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)",
            "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Unwraps a list of parameters gradients\\n\\n    # Returns\\n\\n    `torch.Tensor`\\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\\n    '\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)",
            "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Unwraps a list of parameters gradients\\n\\n    # Returns\\n\\n    `torch.Tensor`\\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\\n    '\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)",
            "def _flatten_tensors(tensors: Sequence[torch.Tensor]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Unwraps a list of parameters gradients\\n\\n    # Returns\\n\\n    `torch.Tensor`\\n        A tensor of shape `(x,)` where `x` is the total number of entires in the gradients.\\n    '\n    views = []\n    for p in tensors:\n        if p.data.is_sparse:\n            view = p.data.to_dense().view(-1)\n        else:\n            view = p.data.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)"
        ]
    }
]