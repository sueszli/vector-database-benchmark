[
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
        "mutated": [
            "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    if False:\n        i = 10\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type='ll', bw='cv_ls', ckertype='gaussian', okertype='wangryzin', ukertype='aitchisonaitken', defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not isinstance(bw, str):\n        bw = np.asarray(bw)\n        if len(bw) != self.k_vars:\n            raise ValueError('bw must have the same dimension as the number of variables.')\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)"
        ]
    },
    {
        "func_name": "_compute_reg_bw",
        "original": "def _compute_reg_bw(self, bw):\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated",
        "mutated": [
            "def _compute_reg_bw(self, bw):\n    if False:\n        i = 10\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated",
            "def _compute_reg_bw(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated",
            "def _compute_reg_bw(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated",
            "def _compute_reg_bw(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated",
            "def _compute_reg_bw(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(bw, str):\n        self._bw_method = 'user-specified'\n        return np.asarray(bw)\n    else:\n        self._bw_method = bw\n        if bw == 'cv_ls':\n            res = self.cv_loo\n        else:\n            res = self.aic_hurvich\n        X = np.std(self.exog, axis=0)\n        h0 = 1.06 * X * self.nobs ** (-1.0 / (4 + np.size(self.exog, axis=1)))\n        func = self.est[self.reg_type]\n        bw_estimated = optimize.fmin(res, x0=h0, args=(func,), maxiter=1000.0, maxfun=1000.0, disp=0)\n        return bw_estimated"
        ]
    },
    {
        "func_name": "_est_loc_linear",
        "original": "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    \"\"\"\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\n\n        Parameters\n        ----------\n        bw : array_like\n            Vector of bandwidth value(s).\n        endog : 1D array_like\n            The dependent variable.\n        exog : 1D or 2D array_like\n            The independent variable(s).\n        data_predict : 1D array_like of length K, where K is the number of variables.\n            The point at which the density is estimated.\n\n        Returns\n        -------\n        D_x : array_like\n            The value of the conditional mean at `data_predict`.\n\n        Notes\n        -----\n        See p. 81 in [1] and p.38 in [2] for the formulas.\n        Unlike other methods, this one requires that `data_predict` be 1D.\n        \"\"\"\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
        "mutated": [
            "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D array_like of length K, where K is the number of variables.\\n            The point at which the density is estimated.\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at `data_predict`.\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas.\\n        Unlike other methods, this one requires that `data_predict` be 1D.\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D array_like of length K, where K is the number of variables.\\n            The point at which the density is estimated.\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at `data_predict`.\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas.\\n        Unlike other methods, this one requires that `data_predict` be 1D.\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D array_like of length K, where K is the number of variables.\\n            The point at which the density is estimated.\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at `data_predict`.\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas.\\n        Unlike other methods, this one requires that `data_predict` be 1D.\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D array_like of length K, where K is the number of variables.\\n            The point at which the density is estimated.\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at `data_predict`.\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas.\\n        Unlike other methods, this one requires that `data_predict` be 1D.\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D array_like of length K, where K is the number of variables.\\n            The point at which the density is estimated.\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at `data_predict`.\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas.\\n        Unlike other methods, this one requires that `data_predict` be 1D.\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False) / float(nobs)\n    ker = ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)"
        ]
    },
    {
        "func_name": "_est_loc_constant",
        "original": "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    \"\"\"\n        Local constant estimator of g(x) in the regression\n        y = g(x) + e\n\n        Parameters\n        ----------\n        bw : array_like\n            Array of bandwidth value(s).\n        endog : 1D array_like\n            The dependent variable.\n        exog : 1D or 2D array_like\n            The independent variable(s).\n        data_predict : 1D or 2D array_like\n            The point(s) at which the density is estimated.\n\n        Returns\n        -------\n        G : ndarray\n            The value of the conditional mean at `data_predict`.\n        B_x : ndarray\n            The marginal effects.\n        \"\"\"\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)",
        "mutated": [
            "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n    '\\n        Local constant estimator of g(x) in the regression\\n        y = g(x) + e\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Array of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D or 2D array_like\\n            The point(s) at which the density is estimated.\\n\\n        Returns\\n        -------\\n        G : ndarray\\n            The value of the conditional mean at `data_predict`.\\n        B_x : ndarray\\n            The marginal effects.\\n        '\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)",
            "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Local constant estimator of g(x) in the regression\\n        y = g(x) + e\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Array of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D or 2D array_like\\n            The point(s) at which the density is estimated.\\n\\n        Returns\\n        -------\\n        G : ndarray\\n            The value of the conditional mean at `data_predict`.\\n        B_x : ndarray\\n            The marginal effects.\\n        '\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)",
            "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Local constant estimator of g(x) in the regression\\n        y = g(x) + e\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Array of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D or 2D array_like\\n            The point(s) at which the density is estimated.\\n\\n        Returns\\n        -------\\n        G : ndarray\\n            The value of the conditional mean at `data_predict`.\\n        B_x : ndarray\\n            The marginal effects.\\n        '\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)",
            "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Local constant estimator of g(x) in the regression\\n        y = g(x) + e\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Array of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D or 2D array_like\\n            The point(s) at which the density is estimated.\\n\\n        Returns\\n        -------\\n        G : ndarray\\n            The value of the conditional mean at `data_predict`.\\n        B_x : ndarray\\n            The marginal effects.\\n        '\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)",
            "def _est_loc_constant(self, bw, endog, exog, data_predict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Local constant estimator of g(x) in the regression\\n        y = g(x) + e\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Array of bandwidth value(s).\\n        endog : 1D array_like\\n            The dependent variable.\\n        exog : 1D or 2D array_like\\n            The independent variable(s).\\n        data_predict : 1D or 2D array_like\\n            The point(s) at which the density is estimated.\\n\\n        Returns\\n        -------\\n        G : ndarray\\n            The value of the conditional mean at `data_predict`.\\n        B_x : ndarray\\n            The marginal effects.\\n        '\n    ker_x = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker_x = np.reshape(ker_x, np.shape(endog))\n    G_numer = (ker_x * endog).sum(axis=0)\n    G_denom = ker_x.sum(axis=0)\n    G = G_numer / G_denom\n    nobs = exog.shape[0]\n    f_x = G_denom / float(nobs)\n    ker_xc = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype='d_gaussian', tosum=False)\n    ker_xc = ker_xc[:, np.newaxis]\n    d_mx = -(endog * ker_xc).sum(axis=0) / float(nobs)\n    d_fx = -ker_xc.sum(axis=0) / float(nobs)\n    B_x = d_mx / f_x - G * d_fx / f_x\n    B_x = (G_numer * d_fx - G_denom * d_mx) / G_denom ** 2\n    return (G, B_x)"
        ]
    },
    {
        "func_name": "aic_hurvich",
        "original": "def aic_hurvich(self, bw, func=None):\n    \"\"\"\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\n\n        Parameters\n        ----------\n        bw : str or array_like\n            See the ``bw`` parameter of `KernelReg` for details.\n\n        Returns\n        -------\n        aic : ndarray\n            The AIC Hurvich criteria, one element for each variable.\n        func : None\n            Unused here, needed in signature because it's used in `cv_loo`.\n\n        References\n        ----------\n        See ch.2 in [1] and p.35 in [2].\n        \"\"\"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic",
        "mutated": [
            "def aic_hurvich(self, bw, func=None):\n    if False:\n        i = 10\n    \"\\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\\n\\n        Parameters\\n        ----------\\n        bw : str or array_like\\n            See the ``bw`` parameter of `KernelReg` for details.\\n\\n        Returns\\n        -------\\n        aic : ndarray\\n            The AIC Hurvich criteria, one element for each variable.\\n        func : None\\n            Unused here, needed in signature because it's used in `cv_loo`.\\n\\n        References\\n        ----------\\n        See ch.2 in [1] and p.35 in [2].\\n        \"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic",
            "def aic_hurvich(self, bw, func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\\n\\n        Parameters\\n        ----------\\n        bw : str or array_like\\n            See the ``bw`` parameter of `KernelReg` for details.\\n\\n        Returns\\n        -------\\n        aic : ndarray\\n            The AIC Hurvich criteria, one element for each variable.\\n        func : None\\n            Unused here, needed in signature because it's used in `cv_loo`.\\n\\n        References\\n        ----------\\n        See ch.2 in [1] and p.35 in [2].\\n        \"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic",
            "def aic_hurvich(self, bw, func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\\n\\n        Parameters\\n        ----------\\n        bw : str or array_like\\n            See the ``bw`` parameter of `KernelReg` for details.\\n\\n        Returns\\n        -------\\n        aic : ndarray\\n            The AIC Hurvich criteria, one element for each variable.\\n        func : None\\n            Unused here, needed in signature because it's used in `cv_loo`.\\n\\n        References\\n        ----------\\n        See ch.2 in [1] and p.35 in [2].\\n        \"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic",
            "def aic_hurvich(self, bw, func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\\n\\n        Parameters\\n        ----------\\n        bw : str or array_like\\n            See the ``bw`` parameter of `KernelReg` for details.\\n\\n        Returns\\n        -------\\n        aic : ndarray\\n            The AIC Hurvich criteria, one element for each variable.\\n        func : None\\n            Unused here, needed in signature because it's used in `cv_loo`.\\n\\n        References\\n        ----------\\n        See ch.2 in [1] and p.35 in [2].\\n        \"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic",
            "def aic_hurvich(self, bw, func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Computes the AIC Hurvich criteria for the estimation of the bandwidth.\\n\\n        Parameters\\n        ----------\\n        bw : str or array_like\\n            See the ``bw`` parameter of `KernelReg` for details.\\n\\n        Returns\\n        -------\\n        aic : ndarray\\n            The AIC Hurvich criteria, one element for each variable.\\n        func : None\\n            Unused here, needed in signature because it's used in `cv_loo`.\\n\\n        References\\n        ----------\\n        See ch.2 in [1] and p.35 in [2].\\n        \"\n    H = np.empty((self.nobs, self.nobs))\n    for j in range(self.nobs):\n        H[:, j] = gpke(bw, data=self.exog, data_predict=self.exog[j, :], ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, var_type=self.var_type, tosum=False)\n    denom = H.sum(axis=1)\n    H = H / denom\n    gx = KernelReg(endog=self.endog, exog=self.exog, var_type=self.var_type, reg_type=self.reg_type, bw=bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    gx = np.reshape(gx, (self.nobs, 1))\n    sigma = ((self.endog - gx) ** 2).sum(axis=0) / float(self.nobs)\n    frac = (1 + np.trace(H) / float(self.nobs)) / (1 - (np.trace(H) + 2) / float(self.nobs))\n    aic = np.log(sigma) + frac\n    return aic"
        ]
    },
    {
        "func_name": "cv_loo",
        "original": "def cv_loo(self, bw, func):\n    \"\"\"\n        The cross-validation function with leave-one-out estimator.\n\n        Parameters\n        ----------\n        bw : array_like\n            Vector of bandwidth values.\n        func : callable function\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\n            (local constant) or ``_est_loc_linear`` (local_linear).\n\n        Returns\n        -------\n        L : float\n            The value of the CV function.\n\n        Notes\n        -----\n        Calculates the cross-validation least-squares function. This function\n        is minimized by compute_bw to calculate the optimal value of `bw`.\n\n        For details see p.35 in [2]\n\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\n\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\n        and :math:`h` is the vector of bandwidths\n        \"\"\"\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
        "mutated": [
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n    '\\n        The cross-validation function with leave-one-out estimator.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values.\\n        func : callable function\\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\\n            (local constant) or ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function.\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares function. This function\\n        is minimized by compute_bw to calculate the optimal value of `bw`.\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        The cross-validation function with leave-one-out estimator.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values.\\n        func : callable function\\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\\n            (local constant) or ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function.\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares function. This function\\n        is minimized by compute_bw to calculate the optimal value of `bw`.\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        The cross-validation function with leave-one-out estimator.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values.\\n        func : callable function\\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\\n            (local constant) or ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function.\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares function. This function\\n        is minimized by compute_bw to calculate the optimal value of `bw`.\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        The cross-validation function with leave-one-out estimator.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values.\\n        func : callable function\\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\\n            (local constant) or ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function.\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares function. This function\\n        is minimized by compute_bw to calculate the optimal value of `bw`.\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        The cross-validation function with leave-one-out estimator.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values.\\n        func : callable function\\n            Returns the estimator of g(x).  Can be either ``_est_loc_constant``\\n            (local constant) or ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function.\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares function. This function\\n        is minimized by compute_bw to calculate the optimal value of `bw`.\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :])[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs"
        ]
    },
    {
        "func_name": "r_squared",
        "original": "def r_squared(self):\n    \"\"\"\n        Returns the R-Squared for the nonparametric regression.\n\n        Notes\n        -----\n        For more details see p.45 in [2]\n        The R-Squared is calculated by:\n\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\n\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\n        points.\n        \"\"\"\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom",
        "mutated": [
            "def r_squared(self):\n    if False:\n        i = 10\n    '\\n        Returns the R-Squared for the nonparametric regression.\\n\\n        Notes\\n        -----\\n        For more details see p.45 in [2]\\n        The R-Squared is calculated by:\\n\\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\\n\\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\\n        points.\\n        '\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom",
            "def r_squared(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the R-Squared for the nonparametric regression.\\n\\n        Notes\\n        -----\\n        For more details see p.45 in [2]\\n        The R-Squared is calculated by:\\n\\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\\n\\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\\n        points.\\n        '\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom",
            "def r_squared(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the R-Squared for the nonparametric regression.\\n\\n        Notes\\n        -----\\n        For more details see p.45 in [2]\\n        The R-Squared is calculated by:\\n\\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\\n\\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\\n        points.\\n        '\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom",
            "def r_squared(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the R-Squared for the nonparametric regression.\\n\\n        Notes\\n        -----\\n        For more details see p.45 in [2]\\n        The R-Squared is calculated by:\\n\\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\\n\\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\\n        points.\\n        '\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom",
            "def r_squared(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the R-Squared for the nonparametric regression.\\n\\n        Notes\\n        -----\\n        For more details see p.45 in [2]\\n        The R-Squared is calculated by:\\n\\n        .. math:: R^{2}=\\\\frac{\\\\left[\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})(\\\\hat{Y_{i}}-\\\\bar{y}\\\\right]^{2}}{\\\\sum_{i=1}^{n}\\n            (Y_{i}-\\\\bar{y})^{2}\\\\sum_{i=1}^{n}(\\\\hat{Y_{i}}-\\\\bar{y})^{2}},\\n\\n        where :math:`\\\\hat{Y_{i}}` is the mean calculated in `fit` at the exog\\n        points.\\n        '\n    Y = np.squeeze(self.endog)\n    Yhat = self.fit()[0]\n    Y_bar = np.mean(Yhat)\n    R2_numer = ((Y - Y_bar) * (Yhat - Y_bar)).sum() ** 2\n    R2_denom = ((Y - Y_bar) ** 2).sum(axis=0) * ((Yhat - Y_bar) ** 2).sum(axis=0)\n    return R2_numer / R2_denom"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, data_predict=None):\n    \"\"\"\n        Returns the mean and marginal effects at the `data_predict` points.\n\n        Parameters\n        ----------\n        data_predict : array_like, optional\n            Points at which to return the mean and marginal effects.  If not\n            given, ``data_predict == exog``.\n\n        Returns\n        -------\n        mean : ndarray\n            The regression result for the mean (i.e. the actual curve).\n        mfx : ndarray\n            The marginal effects, i.e. the partial derivatives of the mean.\n        \"\"\"\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
        "mutated": [
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n    '\\n        Returns the mean and marginal effects at the `data_predict` points.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points at which to return the mean and marginal effects.  If not\\n            given, ``data_predict == exog``.\\n\\n        Returns\\n        -------\\n        mean : ndarray\\n            The regression result for the mean (i.e. the actual curve).\\n        mfx : ndarray\\n            The marginal effects, i.e. the partial derivatives of the mean.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the mean and marginal effects at the `data_predict` points.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points at which to return the mean and marginal effects.  If not\\n            given, ``data_predict == exog``.\\n\\n        Returns\\n        -------\\n        mean : ndarray\\n            The regression result for the mean (i.e. the actual curve).\\n        mfx : ndarray\\n            The marginal effects, i.e. the partial derivatives of the mean.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the mean and marginal effects at the `data_predict` points.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points at which to return the mean and marginal effects.  If not\\n            given, ``data_predict == exog``.\\n\\n        Returns\\n        -------\\n        mean : ndarray\\n            The regression result for the mean (i.e. the actual curve).\\n        mfx : ndarray\\n            The marginal effects, i.e. the partial derivatives of the mean.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the mean and marginal effects at the `data_predict` points.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points at which to return the mean and marginal effects.  If not\\n            given, ``data_predict == exog``.\\n\\n        Returns\\n        -------\\n        mean : ndarray\\n            The regression result for the mean (i.e. the actual curve).\\n        mfx : ndarray\\n            The marginal effects, i.e. the partial derivatives of the mean.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the mean and marginal effects at the `data_predict` points.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points at which to return the mean and marginal effects.  If not\\n            given, ``data_predict == exog``.\\n\\n        Returns\\n        -------\\n        mean : ndarray\\n            The regression result for the mean (i.e. the actual curve).\\n        mfx : ndarray\\n            The marginal effects, i.e. the partial derivatives of the mean.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :])\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)"
        ]
    },
    {
        "func_name": "sig_test",
        "original": "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    \"\"\"\n        Significance test for the variables in the regression.\n\n        Parameters\n        ----------\n        var_pos : sequence\n            The position of the variable in exog to be tested.\n\n        Returns\n        -------\n        sig : str\n            The level of significance:\n\n                - `*` : at 90% confidence level\n                - `**` : at 95% confidence level\n                - `***` : at 99* confidence level\n                - \"Not Significant\" : if not significant\n        \"\"\"\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig",
        "mutated": [
            "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    if False:\n        i = 10\n    '\\n        Significance test for the variables in the regression.\\n\\n        Parameters\\n        ----------\\n        var_pos : sequence\\n            The position of the variable in exog to be tested.\\n\\n        Returns\\n        -------\\n        sig : str\\n            The level of significance:\\n\\n                - `*` : at 90% confidence level\\n                - `**` : at 95% confidence level\\n                - `***` : at 99* confidence level\\n                - \"Not Significant\" : if not significant\\n        '\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig",
            "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Significance test for the variables in the regression.\\n\\n        Parameters\\n        ----------\\n        var_pos : sequence\\n            The position of the variable in exog to be tested.\\n\\n        Returns\\n        -------\\n        sig : str\\n            The level of significance:\\n\\n                - `*` : at 90% confidence level\\n                - `**` : at 95% confidence level\\n                - `***` : at 99* confidence level\\n                - \"Not Significant\" : if not significant\\n        '\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig",
            "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Significance test for the variables in the regression.\\n\\n        Parameters\\n        ----------\\n        var_pos : sequence\\n            The position of the variable in exog to be tested.\\n\\n        Returns\\n        -------\\n        sig : str\\n            The level of significance:\\n\\n                - `*` : at 90% confidence level\\n                - `**` : at 95% confidence level\\n                - `***` : at 99* confidence level\\n                - \"Not Significant\" : if not significant\\n        '\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig",
            "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Significance test for the variables in the regression.\\n\\n        Parameters\\n        ----------\\n        var_pos : sequence\\n            The position of the variable in exog to be tested.\\n\\n        Returns\\n        -------\\n        sig : str\\n            The level of significance:\\n\\n                - `*` : at 90% confidence level\\n                - `**` : at 95% confidence level\\n                - `***` : at 99* confidence level\\n                - \"Not Significant\" : if not significant\\n        '\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig",
            "def sig_test(self, var_pos, nboot=50, nested_res=25, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Significance test for the variables in the regression.\\n\\n        Parameters\\n        ----------\\n        var_pos : sequence\\n            The position of the variable in exog to be tested.\\n\\n        Returns\\n        -------\\n        sig : str\\n            The level of significance:\\n\\n                - `*` : at 90% confidence level\\n                - `**` : at 95% confidence level\\n                - `***` : at 99* confidence level\\n                - \"Not Significant\" : if not significant\\n        '\n    var_pos = np.asarray(var_pos)\n    (ix_cont, ix_ord, ix_unord) = _get_type_pos(self.var_type)\n    if np.any(ix_cont[var_pos]):\n        if np.any(ix_ord[var_pos]) or np.any(ix_unord[var_pos]):\n            raise ValueError('Discrete variable in hypothesis. Must be continuous')\n        Sig = TestRegCoefC(self, var_pos, nboot, nested_res, pivot)\n    else:\n        Sig = TestRegCoefD(self, var_pos, nboot)\n    return Sig.sig"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    \"\"\"Provide something sane to print.\"\"\"\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    'Provide something sane to print.'\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Provide something sane to print.'\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Provide something sane to print.'\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Provide something sane to print.'\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Provide something sane to print.'\n    rpr = 'KernelReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   N = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr"
        ]
    },
    {
        "func_name": "_get_class_vars_type",
        "original": "def _get_class_vars_type(self):\n    \"\"\"Helper method to be able to pass needed vars to _compute_subset.\"\"\"\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)",
        "mutated": [
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KernelReg'\n    class_vars = (self.var_type, self.k_vars, self.reg_type)\n    return (class_type, class_vars)"
        ]
    },
    {
        "func_name": "_compute_dispersion",
        "original": "def _compute_dispersion(self, data):\n    \"\"\"\n        Computes the measure of dispersion.\n\n        The minimum of the standard deviation and interquartile range / 1.349\n\n        References\n        ----------\n        See the user guide for the np package in R.\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\n        a discussion on the measure of dispersion\n        \"\"\"\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)",
        "mutated": [
            "def _compute_dispersion(self, data):\n    if False:\n        i = 10\n    '\\n        Computes the measure of dispersion.\\n\\n        The minimum of the standard deviation and interquartile range / 1.349\\n\\n        References\\n        ----------\\n        See the user guide for the np package in R.\\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\\n        a discussion on the measure of dispersion\\n        '\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)",
            "def _compute_dispersion(self, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the measure of dispersion.\\n\\n        The minimum of the standard deviation and interquartile range / 1.349\\n\\n        References\\n        ----------\\n        See the user guide for the np package in R.\\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\\n        a discussion on the measure of dispersion\\n        '\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)",
            "def _compute_dispersion(self, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the measure of dispersion.\\n\\n        The minimum of the standard deviation and interquartile range / 1.349\\n\\n        References\\n        ----------\\n        See the user guide for the np package in R.\\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\\n        a discussion on the measure of dispersion\\n        '\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)",
            "def _compute_dispersion(self, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the measure of dispersion.\\n\\n        The minimum of the standard deviation and interquartile range / 1.349\\n\\n        References\\n        ----------\\n        See the user guide for the np package in R.\\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\\n        a discussion on the measure of dispersion\\n        '\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)",
            "def _compute_dispersion(self, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the measure of dispersion.\\n\\n        The minimum of the standard deviation and interquartile range / 1.349\\n\\n        References\\n        ----------\\n        See the user guide for the np package in R.\\n        In the notes on bwscaling option in npreg, npudens, npcdens there is\\n        a discussion on the measure of dispersion\\n        '\n    data = data[:, 1:]\n    return _compute_min_std_IQR(data)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
        "mutated": [
            "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    if False:\n        i = 10\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, var_type, reg_type, bw='cv_ls', ckertype='gaussian', ukertype='aitchison_aitken_reg', okertype='wangryzin_reg', censor_val=0, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.var_type = var_type\n    self.data_type = var_type\n    self.reg_type = reg_type\n    self.ckertype = ckertype\n    self.okertype = okertype\n    self.ukertype = ukertype\n    if not (self.ckertype in kernel_func and self.ukertype in kernel_func and (self.okertype in kernel_func)):\n        raise ValueError('user specified kernel must be a supported kernel from statsmodels.nonparametric.kernels.')\n    self.k_vars = len(self.var_type)\n    self.endog = _adjust_shape(endog, 1)\n    self.exog = _adjust_shape(exog, self.k_vars)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.nobs = np.shape(self.exog)[0]\n    self.est = dict(lc=self._est_loc_constant, ll=self._est_loc_linear)\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    self.censor_val = censor_val\n    if self.censor_val is not None:\n        self.censored(censor_val)\n    else:\n        self.W_in = np.ones((self.nobs, 1))\n    if not self.efficient:\n        self.bw = self._compute_reg_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)"
        ]
    },
    {
        "func_name": "censored",
        "original": "def censored(self, censor_val):\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)",
        "mutated": [
            "def censored(self, censor_val):\n    if False:\n        i = 10\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)",
            "def censored(self, censor_val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)",
            "def censored(self, censor_val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)",
            "def censored(self, censor_val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)",
            "def censored(self, censor_val):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.d = (self.endog != censor_val) * 1.0\n    ix = np.argsort(np.squeeze(self.endog))\n    self.sortix = ix\n    self.sortix_rev = np.zeros(ix.shape, int)\n    self.sortix_rev[ix] = np.arange(len(ix))\n    self.endog = np.squeeze(self.endog[ix])\n    self.endog = _adjust_shape(self.endog, 1)\n    self.exog = np.squeeze(self.exog[ix])\n    self.d = np.squeeze(self.d[ix])\n    self.W_in = np.empty((self.nobs, 1))\n    for i in range(1, self.nobs + 1):\n        P = 1\n        for j in range(1, i):\n            P *= ((self.nobs - j) / (float(self.nobs) - j + 1)) ** self.d[j - 1]\n        self.W_in[i - 1, 0] = P * self.d[i - 1] / (float(self.nobs) - i + 1)"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    \"\"\"Provide something sane to print.\"\"\"\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    'Provide something sane to print.'\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Provide something sane to print.'\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Provide something sane to print.'\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Provide something sane to print.'\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Provide something sane to print.'\n    rpr = 'KernelCensoredReg instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    rpr += 'Estimator type: ' + self.reg_type + '\\n'\n    return rpr"
        ]
    },
    {
        "func_name": "_est_loc_linear",
        "original": "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    \"\"\"\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\n\n        Parameters\n        ----------\n        bw : array_like\n            Vector of bandwidth value(s)\n        endog : 1D array_like\n            The dependent variable\n        exog : 1D or 2D array_like\n            The independent variable(s)\n        data_predict : 1D array_like of length K, where K is\n            the number of variables. The point at which\n            the density is estimated\n\n        Returns\n        -------\n        D_x : array_like\n            The value of the conditional mean at data_predict\n\n        Notes\n        -----\n        See p. 81 in [1] and p.38 in [2] for the formulas\n        Unlike other methods, this one requires that data_predict be 1D\n        \"\"\"\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
        "mutated": [
            "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    if False:\n        i = 10\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s)\\n        endog : 1D array_like\\n            The dependent variable\\n        exog : 1D or 2D array_like\\n            The independent variable(s)\\n        data_predict : 1D array_like of length K, where K is\\n            the number of variables. The point at which\\n            the density is estimated\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at data_predict\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas\\n        Unlike other methods, this one requires that data_predict be 1D\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s)\\n        endog : 1D array_like\\n            The dependent variable\\n        exog : 1D or 2D array_like\\n            The independent variable(s)\\n        data_predict : 1D array_like of length K, where K is\\n            the number of variables. The point at which\\n            the density is estimated\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at data_predict\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas\\n        Unlike other methods, this one requires that data_predict be 1D\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s)\\n        endog : 1D array_like\\n            The dependent variable\\n        exog : 1D or 2D array_like\\n            The independent variable(s)\\n        data_predict : 1D array_like of length K, where K is\\n            the number of variables. The point at which\\n            the density is estimated\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at data_predict\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas\\n        Unlike other methods, this one requires that data_predict be 1D\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s)\\n        endog : 1D array_like\\n            The dependent variable\\n        exog : 1D or 2D array_like\\n            The independent variable(s)\\n        data_predict : 1D array_like of length K, where K is\\n            the number of variables. The point at which\\n            the density is estimated\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at data_predict\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas\\n        Unlike other methods, this one requires that data_predict be 1D\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)",
            "def _est_loc_linear(self, bw, endog, exog, data_predict, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Local linear estimator of g(x) in the regression ``y = g(x) + e``.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth value(s)\\n        endog : 1D array_like\\n            The dependent variable\\n        exog : 1D or 2D array_like\\n            The independent variable(s)\\n        data_predict : 1D array_like of length K, where K is\\n            the number of variables. The point at which\\n            the density is estimated\\n\\n        Returns\\n        -------\\n        D_x : array_like\\n            The value of the conditional mean at data_predict\\n\\n        Notes\\n        -----\\n        See p. 81 in [1] and p.38 in [2] for the formulas\\n        Unlike other methods, this one requires that data_predict be 1D\\n        '\n    (nobs, k_vars) = exog.shape\n    ker = gpke(bw, data=exog, data_predict=data_predict, var_type=self.var_type, ckertype=self.ckertype, ukertype=self.ukertype, okertype=self.okertype, tosum=False)\n    ker = W * ker[:, np.newaxis]\n    M12 = exog - data_predict\n    M22 = np.dot(M12.T, M12 * ker)\n    M12 = (M12 * ker).sum(axis=0)\n    M = np.empty((k_vars + 1, k_vars + 1))\n    M[0, 0] = ker.sum()\n    M[0, 1:] = M12\n    M[1:, 0] = M12\n    M[1:, 1:] = M22\n    ker_endog = ker * endog\n    V = np.empty((k_vars + 1, 1))\n    V[0, 0] = ker_endog.sum()\n    V[1:, 0] = ((exog - data_predict) * ker_endog).sum(axis=0)\n    mean_mfx = np.dot(np.linalg.pinv(M), V)\n    mean = mean_mfx[0]\n    mfx = mean_mfx[1:, :]\n    return (mean, mfx)"
        ]
    },
    {
        "func_name": "cv_loo",
        "original": "def cv_loo(self, bw, func):\n    \"\"\"\n        The cross-validation function with leave-one-out\n        estimator\n\n        Parameters\n        ----------\n        bw : array_like\n            Vector of bandwidth values\n        func : callable function\n            Returns the estimator of g(x).\n            Can be either ``_est_loc_constant`` (local constant) or\n            ``_est_loc_linear`` (local_linear).\n\n        Returns\n        -------\n        L : float\n            The value of the CV function\n\n        Notes\n        -----\n        Calculates the cross-validation least-squares\n        function. This function is minimized by compute_bw\n        to calculate the optimal value of bw\n\n        For details see p.35 in [2]\n\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\n\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\n        and :math:`h` is the vector of bandwidths\n        \"\"\"\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
        "mutated": [
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n    '\\n        The cross-validation function with leave-one-out\\n        estimator\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values\\n        func : callable function\\n            Returns the estimator of g(x).\\n            Can be either ``_est_loc_constant`` (local constant) or\\n            ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares\\n        function. This function is minimized by compute_bw\\n        to calculate the optimal value of bw\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        The cross-validation function with leave-one-out\\n        estimator\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values\\n        func : callable function\\n            Returns the estimator of g(x).\\n            Can be either ``_est_loc_constant`` (local constant) or\\n            ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares\\n        function. This function is minimized by compute_bw\\n        to calculate the optimal value of bw\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        The cross-validation function with leave-one-out\\n        estimator\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values\\n        func : callable function\\n            Returns the estimator of g(x).\\n            Can be either ``_est_loc_constant`` (local constant) or\\n            ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares\\n        function. This function is minimized by compute_bw\\n        to calculate the optimal value of bw\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        The cross-validation function with leave-one-out\\n        estimator\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values\\n        func : callable function\\n            Returns the estimator of g(x).\\n            Can be either ``_est_loc_constant`` (local constant) or\\n            ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares\\n        function. This function is minimized by compute_bw\\n        to calculate the optimal value of bw\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs",
            "def cv_loo(self, bw, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        The cross-validation function with leave-one-out\\n        estimator\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            Vector of bandwidth values\\n        func : callable function\\n            Returns the estimator of g(x).\\n            Can be either ``_est_loc_constant`` (local constant) or\\n            ``_est_loc_linear`` (local_linear).\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the CV function\\n\\n        Notes\\n        -----\\n        Calculates the cross-validation least-squares\\n        function. This function is minimized by compute_bw\\n        to calculate the optimal value of bw\\n\\n        For details see p.35 in [2]\\n\\n        .. math:: CV(h)=n^{-1}\\\\sum_{i=1}^{n}(Y_{i}-g_{-i}(X_{i}))^{2}\\n\\n        where :math:`g_{-i}(X_{i})` is the leave-one-out estimator of g(X)\\n        and :math:`h` is the vector of bandwidths\\n        '\n    LOO_X = LeaveOneOut(self.exog)\n    LOO_Y = LeaveOneOut(self.endog).__iter__()\n    LOO_W = LeaveOneOut(self.W_in).__iter__()\n    L = 0\n    for (ii, X_not_i) in enumerate(LOO_X):\n        Y = next(LOO_Y)\n        w = next(LOO_W)\n        G = func(bw, endog=Y, exog=-X_not_i, data_predict=-self.exog[ii, :], W=w)[0]\n        L += (self.endog[ii] - G) ** 2\n    return L / self.nobs"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, data_predict=None):\n    \"\"\"\n        Returns the marginal effects at the data_predict points.\n        \"\"\"\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
        "mutated": [
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n    '\\n        Returns the marginal effects at the data_predict points.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the marginal effects at the data_predict points.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the marginal effects at the data_predict points.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the marginal effects at the data_predict points.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)",
            "def fit(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the marginal effects at the data_predict points.\\n        '\n    func = self.est[self.reg_type]\n    if data_predict is None:\n        data_predict = self.exog\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    N_data_predict = np.shape(data_predict)[0]\n    mean = np.empty((N_data_predict,))\n    mfx = np.empty((N_data_predict, self.k_vars))\n    for i in range(N_data_predict):\n        mean_mfx = func(self.bw, self.endog, self.exog, data_predict=data_predict[i, :], W=self.W_in)\n        mean[i] = np.squeeze(mean_mfx[0])\n        mfx_c = np.squeeze(mean_mfx[1])\n        mfx[i, :] = mfx_c\n    return (mean, mfx)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()",
        "mutated": [
            "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    if False:\n        i = 10\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()",
            "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()",
            "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()",
            "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()",
            "def __init__(self, model, test_vars, nboot=400, nested_res=400, pivot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.nboot = nboot\n    self.nres = nested_res\n    self.test_vars = test_vars\n    self.model = model\n    self.bw = model.bw\n    self.var_type = model.var_type\n    self.k_vars = len(self.var_type)\n    self.endog = model.endog\n    self.exog = model.exog\n    self.gx = model.est[model.reg_type]\n    self.test_vars = test_vars\n    self.pivot = pivot\n    self.run()"
        ]
    },
    {
        "func_name": "run",
        "original": "def run(self):\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()",
        "mutated": [
            "def run(self):\n    if False:\n        i = 10\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()",
            "def run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.test_stat = self._compute_test_stat(self.endog, self.exog)\n    self.sig = self._compute_sig()"
        ]
    },
    {
        "func_name": "_compute_test_stat",
        "original": "def _compute_test_stat(self, Y, X):\n    \"\"\"\n        Computes the test statistic.  See p.371 in [8].\n        \"\"\"\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t",
        "mutated": [
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n    '\\n        Computes the test statistic.  See p.371 in [8].\\n        '\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the test statistic.  See p.371 in [8].\\n        '\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the test statistic.  See p.371 in [8].\\n        '\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the test statistic.  See p.371 in [8].\\n        '\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the test statistic.  See p.371 in [8].\\n        '\n    lam = self._compute_lambda(Y, X)\n    t = lam\n    if self.pivot:\n        se_lam = self._compute_se_lambda(Y, X)\n        t = lam / float(se_lam)\n    return t"
        ]
    },
    {
        "func_name": "_compute_lambda",
        "original": "def _compute_lambda(self, Y, X):\n    \"\"\"Computes only lambda -- the main part of the test statistic\"\"\"\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam",
        "mutated": [
            "def _compute_lambda(self, Y, X):\n    if False:\n        i = 10\n    'Computes only lambda -- the main part of the test statistic'\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam",
            "def _compute_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes only lambda -- the main part of the test statistic'\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam",
            "def _compute_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes only lambda -- the main part of the test statistic'\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam",
            "def _compute_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes only lambda -- the main part of the test statistic'\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam",
            "def _compute_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes only lambda -- the main part of the test statistic'\n    n = np.shape(X)[0]\n    Y = _adjust_shape(Y, 1)\n    X = _adjust_shape(X, self.k_vars)\n    b = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[1]\n    b = b[:, self.test_vars]\n    b = np.reshape(b, (n, len(self.test_vars)))\n    fct = 1.0\n    lam = ((b / fct) ** 2).sum() / float(n)\n    return lam"
        ]
    },
    {
        "func_name": "_compute_se_lambda",
        "original": "def _compute_se_lambda(self, Y, X):\n    \"\"\"\n        Calculates the SE of lambda by nested resampling\n        Used to pivot the statistic.\n        Bootstrapping works better with estimating pivotal statistics\n        but slows down computation significantly.\n        \"\"\"\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda",
        "mutated": [
            "def _compute_se_lambda(self, Y, X):\n    if False:\n        i = 10\n    '\\n        Calculates the SE of lambda by nested resampling\\n        Used to pivot the statistic.\\n        Bootstrapping works better with estimating pivotal statistics\\n        but slows down computation significantly.\\n        '\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda",
            "def _compute_se_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calculates the SE of lambda by nested resampling\\n        Used to pivot the statistic.\\n        Bootstrapping works better with estimating pivotal statistics\\n        but slows down computation significantly.\\n        '\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda",
            "def _compute_se_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calculates the SE of lambda by nested resampling\\n        Used to pivot the statistic.\\n        Bootstrapping works better with estimating pivotal statistics\\n        but slows down computation significantly.\\n        '\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda",
            "def _compute_se_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calculates the SE of lambda by nested resampling\\n        Used to pivot the statistic.\\n        Bootstrapping works better with estimating pivotal statistics\\n        but slows down computation significantly.\\n        '\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda",
            "def _compute_se_lambda(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calculates the SE of lambda by nested resampling\\n        Used to pivot the statistic.\\n        Bootstrapping works better with estimating pivotal statistics\\n        but slows down computation significantly.\\n        '\n    n = np.shape(Y)[0]\n    lam = np.empty(shape=(self.nres,))\n    for i in range(self.nres):\n        ind = np.random.randint(0, n, size=(n, 1))\n        Y1 = Y[ind, 0]\n        X1 = X[ind, :]\n        lam[i] = self._compute_lambda(Y1, X1)\n    se_lambda = np.std(lam)\n    return se_lambda"
        ]
    },
    {
        "func_name": "_compute_sig",
        "original": "def _compute_sig(self):\n    \"\"\"\n        Computes the significance value for the variable(s) tested.\n\n        The empirical distribution of the test statistic is obtained through\n        bootstrapping the sample.  The null hypothesis is rejected if the test\n        statistic is larger than the 90, 95, 99 percentiles.\n        \"\"\"\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig",
        "mutated": [
            "def _compute_sig(self):\n    if False:\n        i = 10\n    '\\n        Computes the significance value for the variable(s) tested.\\n\\n        The empirical distribution of the test statistic is obtained through\\n        bootstrapping the sample.  The null hypothesis is rejected if the test\\n        statistic is larger than the 90, 95, 99 percentiles.\\n        '\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the significance value for the variable(s) tested.\\n\\n        The empirical distribution of the test statistic is obtained through\\n        bootstrapping the sample.  The null hypothesis is rejected if the test\\n        statistic is larger than the 90, 95, 99 percentiles.\\n        '\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the significance value for the variable(s) tested.\\n\\n        The empirical distribution of the test statistic is obtained through\\n        bootstrapping the sample.  The null hypothesis is rejected if the test\\n        statistic is larger than the 90, 95, 99 percentiles.\\n        '\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the significance value for the variable(s) tested.\\n\\n        The empirical distribution of the test statistic is obtained through\\n        bootstrapping the sample.  The null hypothesis is rejected if the test\\n        statistic is larger than the 90, 95, 99 percentiles.\\n        '\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the significance value for the variable(s) tested.\\n\\n        The empirical distribution of the test statistic is obtained through\\n        bootstrapping the sample.  The null hypothesis is rejected if the test\\n        statistic is larger than the 90, 95, 99 percentiles.\\n        '\n    t_dist = np.empty(shape=(self.nboot,))\n    Y = self.endog\n    X = copy.deepcopy(self.exog)\n    n = np.shape(Y)[0]\n    X[:, self.test_vars] = np.mean(X[:, self.test_vars], axis=0)\n    M = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False)).fit()[0]\n    M = np.reshape(M, (n, 1))\n    e = Y - M\n    e = e - np.mean(e)\n    for i in range(self.nboot):\n        ind = np.random.randint(0, n, size=(n, 1))\n        e_boot = e[ind, 0]\n        Y_boot = M + e_boot\n        t_dist[i] = self._compute_test_stat(Y_boot, self.exog)\n    self.t_dist = t_dist\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(t_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(t_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(t_dist, 0.99):\n        sig = '***'\n    return sig"
        ]
    },
    {
        "func_name": "_compute_test_stat",
        "original": "def _compute_test_stat(self, Y, X):\n    \"\"\"Computes the test statistic\"\"\"\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg",
        "mutated": [
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n    'Computes the test statistic'\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the test statistic'\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the test statistic'\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the test statistic'\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg",
            "def _compute_test_stat(self, Y, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the test statistic'\n    dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    n = np.shape(X)[0]\n    model = KernelReg(Y, X, self.var_type, self.model.reg_type, self.bw, defaults=EstimatorSettings(efficient=False))\n    X1 = copy.deepcopy(X)\n    X1[:, self.test_vars] = 0\n    m0 = model.fit(data_predict=X1)[0]\n    m0 = np.reshape(m0, (n, 1))\n    zvec = np.zeros((n, 1))\n    for i in dom_x[1:]:\n        X1[:, self.test_vars] = i\n        m1 = model.fit(data_predict=X1)[0]\n        m1 = np.reshape(m1, (n, 1))\n        zvec += (m1 - m0) ** 2\n    avg = zvec.sum(axis=0) / float(n)\n    return avg"
        ]
    },
    {
        "func_name": "_compute_sig",
        "original": "def _compute_sig(self):\n    \"\"\"Calculates the significance level of the variable tested\"\"\"\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig",
        "mutated": [
            "def _compute_sig(self):\n    if False:\n        i = 10\n    'Calculates the significance level of the variable tested'\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculates the significance level of the variable tested'\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculates the significance level of the variable tested'\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculates the significance level of the variable tested'\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig",
            "def _compute_sig(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculates the significance level of the variable tested'\n    m = self._est_cond_mean()\n    Y = self.endog\n    X = self.exog\n    n = np.shape(X)[0]\n    u = Y - m\n    u = u - np.mean(u)\n    fct1 = (1 - 5 ** 0.5) / 2.0\n    fct2 = (1 + 5 ** 0.5) / 2.0\n    u1 = fct1 * u\n    u2 = fct2 * u\n    r = fct2 / 5 ** 0.5\n    I_dist = np.empty((self.nboot, 1))\n    for j in range(self.nboot):\n        u_boot = copy.deepcopy(u2)\n        prob = np.random.uniform(0, 1, size=(n, 1))\n        ind = prob < r\n        u_boot[ind] = u1[ind]\n        Y_boot = m + u_boot\n        I_dist[j] = self._compute_test_stat(Y_boot, X)\n    sig = 'Not Significant'\n    if self.test_stat > mquantiles(I_dist, 0.9):\n        sig = '*'\n    if self.test_stat > mquantiles(I_dist, 0.95):\n        sig = '**'\n    if self.test_stat > mquantiles(I_dist, 0.99):\n        sig = '***'\n    return sig"
        ]
    },
    {
        "func_name": "_est_cond_mean",
        "original": "def _est_cond_mean(self):\n    \"\"\"\n        Calculates the expected conditional mean\n        m(X, Z=l) for all possible l\n        \"\"\"\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m",
        "mutated": [
            "def _est_cond_mean(self):\n    if False:\n        i = 10\n    '\\n        Calculates the expected conditional mean\\n        m(X, Z=l) for all possible l\\n        '\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m",
            "def _est_cond_mean(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calculates the expected conditional mean\\n        m(X, Z=l) for all possible l\\n        '\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m",
            "def _est_cond_mean(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calculates the expected conditional mean\\n        m(X, Z=l) for all possible l\\n        '\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m",
            "def _est_cond_mean(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calculates the expected conditional mean\\n        m(X, Z=l) for all possible l\\n        '\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m",
            "def _est_cond_mean(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calculates the expected conditional mean\\n        m(X, Z=l) for all possible l\\n        '\n    self.dom_x = np.sort(np.unique(self.exog[:, self.test_vars]))\n    X = copy.deepcopy(self.exog)\n    m = 0\n    for i in self.dom_x:\n        X[:, self.test_vars] = i\n        m += self.model.fit(data_predict=X)[0]\n    m = m / float(len(self.dom_x))\n    m = np.reshape(m, (np.shape(self.exog)[0], 1))\n    return m"
        ]
    }
]