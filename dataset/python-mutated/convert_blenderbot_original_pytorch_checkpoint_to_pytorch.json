[
    {
        "func_name": "rename_state_dict_key",
        "original": "def rename_state_dict_key(k):\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k",
        "mutated": [
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if k == 'embeddings.weight':\n        return 'shared.weight'\n    for (parlai_name, hf_name) in PATTERNS:\n        k = k.replace(parlai_name, hf_name)\n    if k.startswith('encoder'):\n        k = k.replace('.attn', '.self_attn')\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'final_layer_norm')\n    elif k.startswith('decoder'):\n        k = k.replace('norm1', 'self_attn_layer_norm')\n        k = k.replace('norm2', 'encoder_attn_layer_norm')\n        k = k.replace('norm3', 'final_layer_norm')\n    return k"
        ]
    },
    {
        "func_name": "rename_layernorm_keys",
        "original": "def rename_layernorm_keys(sd):\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v",
        "mutated": [
            "def rename_layernorm_keys(sd):\n    if False:\n        i = 10\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v",
            "def rename_layernorm_keys(sd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v",
            "def rename_layernorm_keys(sd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v",
            "def rename_layernorm_keys(sd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v",
            "def rename_layernorm_keys(sd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    keys = ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n    for k in keys:\n        v = sd.pop(k)\n        new_k = k.replace('layernorm_embedding', 'layer_norm')\n        assert new_k not in sd\n        sd[new_k] = v"
        ]
    },
    {
        "func_name": "convert_parlai_checkpoint",
        "original": "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    \"\"\"\n    Copy/paste/tweak model's weights to our BERT structure.\n    \"\"\"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_parlai_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_json_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    model = torch.load(checkpoint_path, map_location='cpu')\n    sd = model['model']\n    cfg = BlenderbotConfig.from_json_file(config_json_path)\n    m = BlenderbotForConditionalGeneration(cfg)\n    valid_keys = m.model.state_dict().keys()\n    failures = []\n    mapping = {}\n    for (k, v) in sd.items():\n        if k in IGNORE_KEYS:\n            continue\n        new_k = rename_state_dict_key(k)\n        if new_k not in valid_keys:\n            failures.append([k, new_k])\n        else:\n            mapping[new_k] = v\n    if cfg.normalize_before:\n        rename_layernorm_keys(sd)\n    m.model.load_state_dict(mapping, strict=True)\n    m.half()\n    m.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]